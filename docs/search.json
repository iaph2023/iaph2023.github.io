[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "I am a sophomore at Pomona College intending to double major in Economics and Math, while also pursuing a minor in Data Science. I am also on the Pomona College swim team. I have been working on my data analysis skills and learning R. I have worked on several TidyTuesday datasets to create visuals.\nCheck out the Data Viz tab on the top left to check out some interesting data visuals."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Ian’s Website",
    "section": "",
    "text": "Welcome to my website! Click the links down below for more information.\nBiography: I am a sophomore at Pomona College intending to double major in Economics and Math, while also pursuing a minor in Data Science. I am also on the Pomona College swim team. I am originally from the San Francisco East Bay Area."
  },
  {
    "objectID": "Lisa Vegetable garden.html",
    "href": "Lisa Vegetable garden.html",
    "title": "Lisa Vegetable Garden",
    "section": "",
    "text": "library(tidyverse)\n\nThis project showcases my first time using the introductory tidyverse data visualization skills that I learned in my data science course. In this section I looked at the Lisa Vegetable Garden data set from tidytuesday. I aimed to explore the data set and create a visualization using the data.\nThe dataset used in this project, Lisa’s Vegetable Garden, is sourced from Tidy Tuesday’s May 28, 2024 release.\nThe Lisa’s Vegetable Garden dataset contains detailed records of Lisa Lendway’s vegetable garden from the summers of 2020 and 2021. The data includes planting, harvesting, and spending details, offering insights into gardening practices and changes between the two years. Originally used in her Introduction to Data Science course at Macalester College, the dataset provides a hands-on way to explore data science concepts.\nTidy Tuesday is a weekly R project encouraging data wrangling, visualization, and sharing insights with the #TidyTuesday community.\nHere is the dataset that I used:\n\ntuesdata &lt;- tidytuesdayR::tt_load(2024, week = 22)\n\nharvest_2020 &lt;- tuesdata$harvest_2020\nharvest_2021 &lt;- tuesdata$harvest_2021\nplanting_2020 &lt;- tuesdata$planting_2020\nplanting_2021 &lt;- tuesdata$planting_2021\nspending_2020 &lt;- tuesdata$spending_2020\nspending_2021 &lt;- tuesdata$spending_2021\n\nHere is a quick glimpse of the data contained in the harvest_2020 dataset.\n\nhead(harvest_2020,5)\n\n# A tibble: 5 × 5\n  vegetable variety          date       weight units\n  &lt;chr&gt;     &lt;chr&gt;            &lt;date&gt;      &lt;dbl&gt; &lt;chr&gt;\n1 lettuce   reseed           2020-06-06     20 grams\n2 radish    Garden Party Mix 2020-06-06     36 grams\n3 lettuce   reseed           2020-06-08     15 grams\n4 lettuce   reseed           2020-06-09     10 grams\n5 radish    Garden Party Mix 2020-06-11     67 grams\n\n\nThis code chuck creates a new data set called harvest_2020_avg which I use in my visualization later. I find the average harvest weight across all months in the 2020 harvest season.\n\n# Create a new dataset with average harvest weight per month\nharvest_2020_avg &lt;- harvest_2020 |&gt;\n  # Add a new column 'month', extracting the year and month from the 'date' column\n  mutate(month = as.Date(format(as.Date(date), \"%Y-%m-01\"))) |&gt; \n  # Group the data by the 'month' column\n  group_by(month) |&gt;\n  # Calculate the average weight for each month, ignoring missing values (NA)\n  summarize(avg_weight = mean(weight, na.rm = TRUE))\n\n# Display the resulting dataset\nharvest_2020_avg\n\n# A tibble: 5 × 2\n  month      avg_weight\n  &lt;date&gt;          &lt;dbl&gt;\n1 2020-06-01       78.8\n2 2020-07-01      219. \n3 2020-08-01      473. \n4 2020-09-01     1128. \n5 2020-10-01      959. \n\n\nBelow you can see my code and data visualization of Lisa’s Vegetable Garden. The graph represents the average weight of vegetables harvested during each month of the 2020 harvest.\n\n# Create a line plot of average vegetable weights harvested per month\nggplot(harvest_2020_avg, aes(x = month, y = avg_weight)) +  # Set up ggplot with dataset and aesthetic mappings\n  geom_line() +  # Add a line connecting the average weights for each month\n  geom_point() +  # Add points at each month's average weight for emphasis\n  labs(\n    title = \"Average Weight of Vegetables Harvested per Month in 2020 Season\",  # Set the plot title\n    x = \"Month\",  # Label for the x-axis (Month)\n    y = \"Average Weight (grams)\"  # Label for the y-axis (Average Weight in grams)\n  )\n\n\n\n\n\n\n\n\nAs we can see, the graph shows the average weight of vegetables harvested per month during the 2020 season. Harvest weights increased steadily from June, peaked in September, and then slightly declined in October, highlighting a clear seasonal trend in vegetable yields."
  },
  {
    "objectID": "Carbon Emissions.html",
    "href": "Carbon Emissions.html",
    "title": "Carbon Emissions",
    "section": "",
    "text": "library(tidyverse)\n\nThis project showcases my use of tidyverse data wrangling and visualization skills. In this section, I analyze the Carbon Emissions dataset from TidyTuesday to explore trends in emissions over time and create a visualization.\nThe dataset used in this project, Carbon Emissions, is sourced from Tidy Tuesday’s May 21, 2024 release.\nThe Carbon Emissions dataset includes historical emissions data compiled by Carbon Majors. This database contains emissions data from 122 of the world’s largest fossil fuel and cement producers, spanning from 1854 to the present. It tracks both operational emissions and emissions resulting from the combustion of marketed products, offering valuable insights into global carbon trends.\nTidy Tuesday is a weekly R project encouraging data exploration, visualization, and sharing insights with the #TidyTuesday community.\nHere is the data set that I used:\n\ntuesdata &lt;- tidytuesdayR::tt_load(2024, week = 21)\n\nemissions &lt;- tuesdata$emissions\n\nBelow is a quick glimpse of the data contained in the emissions dataset:\n\nhead(emissions, 5)\n\n# A tibble: 5 × 7\n   year parent_entity     parent_type commodity production_value production_unit\n  &lt;dbl&gt; &lt;chr&gt;             &lt;chr&gt;       &lt;chr&gt;                &lt;dbl&gt; &lt;chr&gt;          \n1  1962 Abu Dhabi Nation… State-owne… Oil & NGL            0.913 Million bbl/yr \n2  1962 Abu Dhabi Nation… State-owne… Natural …            1.84  Bcf/yr         \n3  1963 Abu Dhabi Nation… State-owne… Oil & NGL            1.83  Million bbl/yr \n4  1963 Abu Dhabi Nation… State-owne… Natural …            4.42  Bcf/yr         \n5  1964 Abu Dhabi Nation… State-owne… Oil & NGL            7.3   Million bbl/yr \n# ℹ 1 more variable: total_emissions_MtCO2e &lt;dbl&gt;\n\n\nThis code chunk creates a new dataset summarizing the average carbon emissions per year. I use this summary data to generate the visualization below:\n\nsummary_data &lt;- emissions |&gt;\n  select(year, total_emissions_MtCO2e) |&gt;  # Select relevant columns\n  group_by(year) |&gt;  # Group by year\n  summarize(ave_emissions = mean(total_emissions_MtCO2e, na.rm = TRUE))  # Calculate the average emissions, ignoring NAs\n\nsummary_data\n\n# A tibble: 169 × 2\n    year ave_emissions\n   &lt;dbl&gt;         &lt;dbl&gt;\n 1  1854        0.0331\n 2  1855        0.0430\n 3  1856        0.0529\n 4  1857        0.0615\n 5  1858        0.0701\n 6  1859        0.0787\n 7  1860        0.0873\n 8  1861        0.0959\n 9  1862        0.105 \n10  1863        0.113 \n# ℹ 159 more rows\n\n\nBelow you can see my code and data visualization of Carbon Emissions overtime in America. The graph shows the increase in Carbon Emissions from 1850-Present Day.\n\nggplot(summary_data, aes(x = year, y = ave_emissions)) +\n  geom_point() +  # Add points for each year's emissions\n  geom_smooth(se = FALSE) +  # Add a smooth trend line without confidence intervals\n  labs(\n    x = \"Year\",  # Label for the x-axis\n    y = \"Average Carbon Emissions (MtCO2e)\",  # Label for the y-axis\n    title = \"Average Carbon Emissions per Year\"  # Set the plot title\n  )\n\n\n\n\n\n\n\n\nBy analyzing this dataset, I gained insights into how carbon emissions have changed over time, reflecting industrialization and energy consumption patterns. This project complements my work with the Lisa’s Vegetable Garden dataset, demonstrating a broader application of data science skills to real-world datasets."
  },
  {
    "objectID": "Mini Project 2.html",
    "href": "Mini Project 2.html",
    "title": "Mini Project 2",
    "section": "",
    "text": "test here\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n\n\ntuesdata &lt;- tidytuesdayR::tt_load('2021-04-20')\ntuesdata &lt;- tidytuesdayR::tt_load(2021, week = 17)\n\nnetflix &lt;- tuesdata$netflix\n\n\nnetflix\n\n# A tibble: 7,787 × 12\n   show_id type    title director   cast  country date_added release_year rating\n   &lt;chr&gt;   &lt;chr&gt;   &lt;chr&gt; &lt;chr&gt;      &lt;chr&gt; &lt;chr&gt;   &lt;chr&gt;             &lt;dbl&gt; &lt;chr&gt; \n 1 s1      TV Show 3%    &lt;NA&gt;       João… Brazil  August 14…         2020 TV-MA \n 2 s2      Movie   7:19  Jorge Mic… Demi… Mexico  December …         2016 TV-MA \n 3 s3      Movie   23:59 Gilbert C… Tedd… Singap… December …         2011 R     \n 4 s4      Movie   9     Shane Ack… Elij… United… November …         2009 PG-13 \n 5 s5      Movie   21    Robert Lu… Jim … United… January 1…         2008 PG-13 \n 6 s6      TV Show 46    Serdar Ak… Erda… Turkey  July 1, 2…         2016 TV-MA \n 7 s7      Movie   122   Yasir Al … Amin… Egypt   June 1, 2…         2019 TV-MA \n 8 s8      Movie   187   Kevin Rey… Samu… United… November …         1997 R     \n 9 s9      Movie   706   Shravan K… Divy… India   April 1, …         2019 TV-14 \n10 s10     Movie   1920  Vikram Bh… Rajn… India   December …         2008 TV-MA \n# ℹ 7,777 more rows\n# ℹ 3 more variables: duration &lt;chr&gt;, listed_in &lt;chr&gt;, description &lt;chr&gt;\n\n\n\n# Distribution of content type\nnetflix_type_count &lt;- netflix |&gt;\n  summarize(n())\n\nnetflix_type_count\n\n# A tibble: 1 × 1\n  `n()`\n  &lt;int&gt;\n1  7787\n\n# Distribution of content release year\n#netflix_year_count &lt;- netflix |&gt;\n#  count(release_year)\n#netflix_year_"
  },
  {
    "objectID": "Mini Project 2 copy.html",
    "href": "Mini Project 2 copy.html",
    "title": "Mini Project 2",
    "section": "",
    "text": "This is my second project which centers on analyzing the Netflix Titles dataset. The focus in this project is to utilize piping to better organize the data. In this project I made three different visualizations. The dataset provides insights into the types of content (movies and TV shows), their release years, associated metadata (e.g., directors, actors, countries of production), and trends across the Netflix platform.\nThis analysis uses data from the Netflix Titles Data Source, which is available through the TidyTuesday github repository.\nExploratory Data Analysis To introduce the dataset, I begin with a simple exploration of key variables:\n\nlibrary(tidyverse)\ntuesdata &lt;- tidytuesdayR::tt_load('2021-04-20')\nnetflix &lt;- tuesdata$netflix\n\nhead(netflix,5)\n\n# A tibble: 5 × 12\n  show_id type    title director    cast  country date_added release_year rating\n  &lt;chr&gt;   &lt;chr&gt;   &lt;chr&gt; &lt;chr&gt;       &lt;chr&gt; &lt;chr&gt;   &lt;chr&gt;             &lt;dbl&gt; &lt;chr&gt; \n1 s1      TV Show 3%    &lt;NA&gt;        João… Brazil  August 14…         2020 TV-MA \n2 s2      Movie   7:19  Jorge Mich… Demi… Mexico  December …         2016 TV-MA \n3 s3      Movie   23:59 Gilbert Ch… Tedd… Singap… December …         2011 R     \n4 s4      Movie   9     Shane Acker Elij… United… November …         2009 PG-13 \n5 s5      Movie   21    Robert Luk… Jim … United… January 1…         2008 PG-13 \n# ℹ 3 more variables: duration &lt;chr&gt;, listed_in &lt;chr&gt;, description &lt;chr&gt;\n\n\nA quick overview of the dataset reveals that the majority of content is categorized as either “Movies” or “TV Shows.” Below is a bar chart illustrating the distribution:\n\n# Distribution of content type\nnetflix_type_count &lt;- netflix |&gt;\n  count(type)\n\nggplot(netflix_type_count, aes(x = type, y = n, fill = type)) +\n  geom_bar(stat = \"identity\") +\n  labs(\n    title = \"Distribution of Movies vs TV Shows\",\n    x = \"Content Type\",\n    y = \"Count\"\n  )\n\n\n\n\n\n\n\n\nHere we can see data frames for the amount of movies and TV shows on Netflix from each release year and the amount of Movies and TV shows on Netflix. We can see that there are around double the amount of movies compared to TV shows. There seems to be a trend of more modern TV shows and movies on Netflix.\nContent Trends Over Time Analyzing release years shows a clear trend: Netflix’s library is heavily skewed toward modern content, with the majority of titles released after 2000. The following line chart visualizes the distribution of release years:\n\n# Distribution of release years\nnetflix_year_count &lt;- netflix |&gt;\n  count(release_year) |&gt;\n  arrange(desc(n))\n\nggplot(netflix_year_count, aes(x = release_year, y = n)) +\n  geom_line() +\n  labs(\n    title = \"Netflix Content Release Year Distribution\",\n    x = \"Release Year\",\n    y = \"Count\"\n  )\n\n\n\n\n\n\n\n\nFrom the line chart we can see that movies after 2000 dominate Netflix’s discography.\nNext I attempted to explore linguistic patterns in Netflix titles. I used str functions str_detect() and str_to_lower() to look for a popular word. Used regular expression “love” to look for the word love in movie and TV titles.\n\n# Count titles containing the word \"love\"\nlove_count &lt;- netflix |&gt;\n  filter(str_detect(str_to_lower(title), \"love\")) |&gt;\n  summarise(count = n())\n\nlove_count\n\n# A tibble: 1 × 1\n  count\n  &lt;int&gt;\n1   175\n\n\nUses regular expression “life|world|death” to search for more possible popular words in move/tv show titles.\n\n# Titles containing the words \"life\", \"world\", or \"death\"\nkeywords_titles &lt;- netflix |&gt;\n  filter(str_detect(str_to_lower(title), \"life|world|death\"))\n\nhead(keywords_titles,10)\n\n# A tibble: 10 × 12\n   show_id type    title   director cast  country date_added release_year rating\n   &lt;chr&gt;   &lt;chr&gt;   &lt;chr&gt;   &lt;chr&gt;    &lt;chr&gt; &lt;chr&gt;   &lt;chr&gt;             &lt;dbl&gt; &lt;chr&gt; \n 1 s150    Movie   A Beau… Andrew … Qi S… China,… December …         2011 TV-14 \n 2 s195    Movie   A Life… Francis… &lt;NA&gt;  Argent… March 20,…         2020 TV-14 \n 3 s310    Movie   Addict… Thierry… Anto… France  June 12, …         2014 TV-14 \n 4 s325    TV Show After … &lt;NA&gt;     Rick… United… April 24,…         2020 TV-MA \n 5 s379    Movie   Alex F… Alex Dí… Alex… Mexico  January 2…         2020 TV-MA \n 6 s399    TV Show Alien … &lt;NA&gt;     &lt;NA&gt;  United… December …         2020 TV-PG \n 7 s421    Movie   All th… Yibrán … Háns… Mexico  January 3…         2020 TV-14 \n 8 s530    Movie   Animal… Han Yan  Li Y… China   September…         2018 TV-MA \n 9 s544    TV Show Anothe… &lt;NA&gt;     Kate… United… July 25, …         2019 TV-MA \n10 s559    TV Show Apache… &lt;NA&gt;     Balt… Argent… August 16…         2019 TV-MA \n# ℹ 3 more variables: duration &lt;chr&gt;, listed_in &lt;chr&gt;, description &lt;chr&gt;\n\n\nThen I proceeded to create a count for each of the selected words.\n\n# Count occurrences of each word \"life\", \"world\", and \"death\" in the titles\nword_count &lt;- netflix |&gt;\n  mutate(title_lower = str_to_lower(title)) |&gt;\n  \n  summarise(\n    life_count = sum(str_count(title_lower, \"life\")),\n    world_count = sum(str_count(title_lower, \"world\")),\n    death_count = sum(str_count(title_lower, \"death\"))\n  )\n\n\nword_count\n\n# A tibble: 1 × 3\n  life_count world_count death_count\n       &lt;int&gt;       &lt;int&gt;       &lt;int&gt;\n1         79          77          24\n\n\nThe following code chucks perform three different tasks of counting occurrences of directors, actors, and countries, and then displaying the top 10 most popular in each category. It filters out missing or empty values and splits data where necessary (such as for actors and countries with multiple entries separated by commas).\n\n# Count the number of occurrences of each director, excluding missing values\npopular_directors &lt;- netflix |&gt;\n  filter(!is.na(director) & director != \"\") |&gt;\n  count(director) |&gt;\n  arrange(desc(n))\n\n# View the top 10 most popular directors\nhead(popular_directors, 10)\n\n# A tibble: 10 × 2\n   director                   n\n   &lt;chr&gt;                  &lt;int&gt;\n 1 Raúl Campos, Jan Suter    18\n 2 Marcus Raboy              16\n 3 Jay Karas                 14\n 4 Cathy Garcia-Molina       13\n 5 Jay Chapman               12\n 6 Martin Scorsese           12\n 7 Youssef Chahine           12\n 8 Steven Spielberg          10\n 9 David Dhawan               9\n10 Hakan Algül                8\n\n# Separate the cast into individual actors and count their occurrences\npopular_actors &lt;- netflix |&gt;\n  filter(!is.na(cast) & cast != \"\") |&gt;\n  separate_rows(cast, sep = \", \") |&gt;\n  count(cast) |&gt;\n  arrange(desc(n))\n\n# View the top 10 most popular actors\nhead(popular_actors, 10)\n\n# A tibble: 10 × 2\n   cast                 n\n   &lt;chr&gt;            &lt;int&gt;\n 1 Anupam Kher         42\n 2 Shah Rukh Khan      35\n 3 Naseeruddin Shah    30\n 4 Om Puri             30\n 5 Akshay Kumar        29\n 6 Takahiro Sakurai    29\n 7 Amitabh Bachchan    27\n 8 Boman Irani         27\n 9 Paresh Rawal        27\n10 Yuki Kaji           27\n\n# Separate the country column into individual countries and count their occurrences\npopular_countries &lt;- netflix |&gt;\n  filter(!is.na(country) & country != \"\") |&gt;\n  separate_rows(country, sep = \", \") |&gt;\n  count(country) |&gt;\n  arrange(desc(n))\n\n# View the top 10 most popular countries\nhead(popular_countries, 10)\n\n# A tibble: 10 × 2\n   country            n\n   &lt;chr&gt;          &lt;int&gt;\n 1 United States   3296\n 2 India            990\n 3 United Kingdom   722\n 4 Canada           412\n 5 France           349\n 6 Japan            287\n 7 Spain            215\n 8 South Korea      212\n 9 Germany          199\n10 Mexico           154\n\n\nIt was very interesting to see the large amount of Indian actors appearing in tv shows/movies compared to the total amount of Indian movies. Perhaps there is a fewer supply of popular actors in Bollywood compared to Hollywood in the United States.\nFinally, I created a table that has the top 10 most popular words in Netflix titles. I excluded common conjunction words such as “the”, “and”, or “of”. I also used the regular expressions \\d and \\s+ to filter out digits and whitespace.\n\nnetflix_lowercase_titles &lt;- netflix |&gt;\n  mutate(lower_title = str_to_lower(title))\n\n#Use regular expressions to extract words and exclude those containing digits\nnetflix_words &lt;- netflix_lowercase_titles |&gt;\n  separate_rows(lower_title, sep = \"\\\\s+\") |&gt;\n  filter(!str_detect(lower_title, \"\\\\d\"))\n\n#Remove common words that are not of interest\ncommon_words &lt;- c(\"the\", \"and\", \"in\", \"of\", \"to\", \"a\", \"is\", \"for\", \"with\", \"on\", \"at\", \"by\", \"an\", \"from\", \"i\", \"&\")\n\nnetflix_words_final &lt;- netflix_words |&gt;\n  filter(!is.element(lower_title, common_words))\n\n#Count the frequency of each word\npopular_words &lt;- netflix_words_final |&gt;\n  count(lower_title, sort = TRUE)\n\nhead(popular_words, 10)\n\n# A tibble: 10 × 2\n   lower_title     n\n   &lt;chr&gt;       &lt;int&gt;\n 1 love          134\n 2 my            127\n 3 christmas      77\n 4 man            71\n 5 you            70\n 6 story          65\n 7 life           59\n 8 world          59\n 9 little         58\n10 one            53\n\n\nLastly, I created a bar chart from the previous data frame in order to visulize the top 10 most common words in Netflix titles.\n\npopular_words |&gt;\n  head(10) |&gt;\n  ggplot(aes(x = reorder(lower_title, n), y = n)) +\n  geom_bar(stat = \"identity\", fill = \"green\") +\n  coord_flip() +\n  labs(title = \"Top 10 Most Common Words in Netflix Titles\",\n       x = \"Words\",\n       y = \"Frequency\")\n\n\n\n\n\n\n\n\nMy early prediction of “love”, “life”, “world” all being popular words in titles turned out to be true. However, “death” did not make the top 10.\nConclusion This project was a great opportunity to practice and build my string manipulation and regular expression skills. By working with the Netflix Titles dataset, I applied several str_*() functions and regular expressions to clean and analyze text data, which was a valuable hands-on experience in managing and extracting important information from more unstructured data. Creating visualizations helped reinforce the importance of organizing data effectively for clear storytelling and making results accessible. This project also revealed some interesting trends in Netflix’s content catalog. The analysis revealed that Netflix’s library is dominated by modern content, with movies significantly outnumbering TV shows. Common themes in titles included “love,” “life,” and “world,” while “death” was less frequent. Indian actors appeared frequently compared to the smaller number of Indian movies, highlighting unique industry dynamics."
  },
  {
    "objectID": "MiniProject3.html",
    "href": "MiniProject3.html",
    "title": "Mini Project 3 Final",
    "section": "",
    "text": "Analysis Plan:\nTo test whether home-court advantage has a measurable effect on win percentages, we’ll calculate the observed difference in median win percentage between home and away games across all NCAA men’s basketball teams in our dataset. To determine if this observed difference could happen by chance, we’ll use a permutation test. Specifically, we’ll shuffle the “home” and “away” labels for each team multiple times and calculate the median difference in win percentages after each shuffle. By comparing our observed difference to this distribution of permuted differences, we can assess whether any observed effect is statistically significant.\nThe result will show whether the median win percentage difference between home and away games is greater than what we’d expect by random chance alone. A low p-value would suggest that the difference is meaningful and likely due to a true home-court advantage.\nData Source:\nThe data used in this analysis comes from a Kaggle dataset by Nishaan Amin, titled March Madness Data, which includes statistics on NCAA men’s basketball tournament teams from 2008 to 2024 (excluding 2020, as the tournament was canceled due to COVID-19). Specifically, we used two files: Barttorvik Home.csv and Barttorvik Away.csv, which provide team statistics for home and away games. These files were sourced from Barttorvik’s college basketball data. The full dataset is available on Kaggle: Home file and Away file.\nBelow is the original data:\n\nlibrary(tidyverse)\n# Original Data\nhome_data &lt;- read.csv(\"Barttorvik Home.csv\")\naway_data &lt;- read.csv(\"Barttorvik Away.csv\")\n\nI loaded the data files, which provided team names and their respective winning percentages (WIN.) for home and away games. The data required some cleaning and restructuring to make it suitable for analysis.\nVisualizing the Original Data Before diving into the analysis, it’s important to understand how win percentages differ between home and away games. To do this, I created a boxplot that compares the distributions of win percentages for home and away games. The boxplot provides a summary of the data by showing the median, interquartile range (IQR), and potential outliers for each location.\nBy visualizing the data in this way, we can see how performance at home differs from performance away, highlighting trends in win percentage distributions and helping to establish a foundation for further analysis. Below is the resulting boxplot.\n\n# Boxplot comparing home and away win percentages\ncombined_data &lt;- bind_rows(\n  home_data |&gt; \n    mutate(location = \"Home\"),\n  away_data |&gt; \n    mutate(location = \"Away\")\n)\n\nggplot(combined_data, aes(x = location, y = WIN., fill = location)) +\n  geom_boxplot() +\n  labs(\n    title = \"Distribution of Win Percentages by Location\",\n    x = \"Game Location\",\n    y = \"Win Percentage\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\nWe can see that the boxplot shows that teams tend to perform better at home than away, as indicated by the higher median win percentage for home games. The distribution of home win percentages is also wider, with more variability compared to away games, which tend to cluster around lower values. This suggests a significant home-court advantage in NCAA basketball games.\nTo further explore this trend and prepare for a more detailed analysis, I needed to clean and organize the data. By calculating summary statistics and structuring the data appropriately, I ensured it was ready for robust hypothesis testing and visualization. Below are the steps I took to wrangle the data.\nDate Wrangling Here, I selected the relevant columns, renamed them for clarity, and added a column to indicate whether the data corresponded to home or away games. I also grouped the data by team and location to calculate the median win percentage for each group.\n\n#Reload original data to avoid values getting changed\nhome_data &lt;- read.csv(\"Barttorvik Home.csv\")\naway_data &lt;- read.csv(\"Barttorvik Away.csv\")\n\n# Select relevant columns and rename for clarity\n# Add a \"location\" column to distinguish between home and away games\nhome_data &lt;- home_data |&gt;\n  select(TEAM, WIN.) |&gt;\n  rename(win_pct = WIN.) |&gt;\n  mutate(location = \"home\")\n\naway_data &lt;- away_data |&gt;\n  select(TEAM, WIN.) |&gt;\n  rename(win_pct = WIN.) |&gt;\n  mutate(location = \"away\")\n\n#Group by TEAM and Location, Calculate Median Win Percentage\nhome_median &lt;- home_data |&gt;\n  group_by(TEAM, location) |&gt;\n  summarise(median_win_pct = median(win_pct, na.rm = TRUE))\n\naway_median &lt;- away_data |&gt;\n  group_by(TEAM, location) |&gt;\n  summarise(median_win_pct = median(win_pct, na.rm = TRUE))\n\n# Combine Home and Away Results\ncombined_median_data &lt;- bind_rows(home_median, away_median)\n\nHere is a quick summary of the results found above, sorted by location.\n\n# Calculate the median win percentages for home and away\nlocation_medians &lt;- combined_median_data |&gt;\n  group_by(location) |&gt;\n  summarise(median_win_pct = median(median_win_pct, na.rm = TRUE))\n\n# Print the results\nlocation_medians\n\n# A tibble: 2 × 2\n  location median_win_pct\n  &lt;chr&gt;             &lt;dbl&gt;\n1 away               54.5\n2 home               85.7\n\n\nNext, we can calculate the observed difference in median win percentage. This will show us the perceved advantage of playing at home compared to playing away.\n\n#Calculate the Observed Difference in Median Win Percentage\nobserved_diff &lt;- combined_median_data |&gt;\n  group_by(location) |&gt;\n  summarise(median_win_pct = median(median_win_pct, na.rm = TRUE)) |&gt;\n  summarise(diff = diff(median_win_pct)) |&gt;\n  pull(diff)\n\ncat(\"Observed Difference in Median Win Percentage (Home - Away):\", observed_diff, \"\\n\")\n\nObserved Difference in Median Win Percentage (Home - Away): 31.16884 \n\n\nFrom here we get a value of around 31%. This means that for NCAA mens basketball from 2008-2024 there was 31% more wins when playing at home compared to away.\nIn order to see if this result is statistically significant I chose to perform a hypothesis test using a permutation function. Using 10000 random samples I shuffled the location of each team in order to make the win percentage uncorrelated with location. We then can compare our original observed difference of 31% to the distribution of randomized outcomes. This will return a two sided P-value.\n\n# Define Permutation Function (Shuffling location within each team)\ncalculate_permutation &lt;- function(data) {\n  data |&gt;\n    group_by(TEAM) |&gt;\n    mutate(location = sample(location, replace = FALSE)) |&gt;\n    group_by(location) |&gt;\n    summarise(median_win_pct = median(median_win_pct, na.rm = TRUE)) |&gt;\n    summarise(diff = diff(median_win_pct)) |&gt;\n    pull(diff)\n}\n#Permutation Test\nnum_permutations &lt;- 10000\nperm_results &lt;- map_dbl(1:num_permutations, ~ calculate_permutation(combined_median_data))\n\n# Calculate the two-sided p-value\np_value &lt;- mean(abs(perm_results) &gt;= abs(observed_diff))\ncat(\"Two-Sided P-value:\", p_value, \"\\n\")\n\nTwo-Sided P-value: 0 \n\n\nA P-value of 0 implies that it is statistically impossible for the observed difference to have occurred by chance. Or in other words, in the 10000 samples that we conducted none of the observed differences were greater than 31%.\nFinally, I used a histogram to visulize the permuted distribution and compared it to the observed difference.\n\n# Histogram of Permuted Differences with the Observed Difference\nggplot(data.frame(perm_results), aes(x = perm_results)) +\n  geom_histogram(bins = 30, color = \"black\", fill = \"skyblue\") +\n  geom_vline(xintercept = observed_diff, color = \"red\", linetype = \"dashed\", linewidth = 1.2) +\n  labs(\n    title = \"Permutation Test: Distribution of Permuted Differences\",\n    x = \"Difference in Median Win Percentage (Home  - Away)\",\n    y = \"Frequency\"\n  )\n\n\n\n\n\n\n\n\nExplanation of Plot:\nThe histogram shows the distribution of permuted differences in median win percentages between home and away games, simulating what we’d expect to see under the null hypothesis (no real difference between home and away performance). Each bar represents the frequency of permuted differences from our 1,000 random shuffles of the home and away labels. The red dashed line on the far right marks the observed difference in median win percentage, which is approximately 31.17%.\nSummary of Findings:\nThe observed difference in median win percentages between home and away games is 31.17%, indicating that teams tend to perform better at home by a substantial margin. The two-sided p-value of 0 confirms that this observed difference is highly statistically significant. None of the permuted differences reached or exceeded the observed value, strongly suggesting that the observed home-court advantage is not due to random variation.\nThis analysis provides strong evidence that playing at home has a positive and significant impact on NCAA teams’ win percentages. Factors such as crowd support, familiarity with the environment, and the absence of travel fatigue may contribute to this advantage. The findings show that home-court advantage is likely a real and influential factor in game outcomes.\nDiscussion: Generalizability of Results This analysis highlights a significant home-court advantage in NCAA men’s basketball, with a 31% higher median win percentage for home games compared to away games. While this result is statistically significant within the scope of the dataset, it is important to consider its generalizability. The dataset spans 2008–2024 and includes a subset of NCAA teams. These findings probably could be extended to other NCAA teams under similar conditions but may not fully capture the variability across different leagues, sports, or time periods. Further research is needed to determine whether the magnitude of home-court advantage is consistent across other contexts."
  },
  {
    "objectID": "Project4_SQL.html",
    "href": "Project4_SQL.html",
    "title": "Project 4 - SQL",
    "section": "",
    "text": "Introduction\nThere are two main goals of this project. First, to recreate Figure 1 from Voss (2020) using SQL to query the Wideband Acoustic Immittance (WAI) Database. This requires calculating mean absorbance values for 12 selected studies and producing a correctly labeled plot in R. Second, to explore a study in the database where subjects of varying age categories, were enrolled. For this study, I will analyze frequency vs. mean absorption by age category and create an informative plot to visualize the data.\nThe SQL queries handle data filtering, aggregation, and JOIN operations to generate datasets ready for visualization in R. The analysis aims to demonstrate proficiency in both SQL for data wrangling and R for plotting.\n\n\nSources\nThis project uses data from the Wideband Acoustic Immittance (WAI) Database hosted by Smith College, which provides WAI ear measurements published in peer-reviewed articles. The data was accessed via SQL queries for analysis.\nThe analysis reproduces Figure 1 from the article:\nVoss, Susan E. Ph.D. (2019). Resource Review. Ear and Hearing, 40(6), p. 1481, November/December 2019.\nDOI: 10.1097/AUD.0000000000000790\nAddtionally, I will provide the sources for both the Abur 2014 and Hunter 2016 as I looked more in depth into their datasets:\nAbur, D., Horton, N. J., & Voss, S. E. (2014). Wideband acoustic immittance measures in normal-hearing adults: Test-retest reliability and effects of ear-canal pressurization. Hearing Research, 316, 23–31. DOI Link\nHunter, L. L., Monson, B. B., Prieve, B. A., & Fitzgerald, T. S. (2016). NICU graduates and wideband absorbance: A comparison of normative data. Hearing Research, 342, 72–80. DOI Link\n\nEstablish connection to the database and also create variables for needed tables for ease of use.\n\n#Establish Connection\nlibrary(RMariaDB)\nlibrary(tidyverse)\ncon_wai &lt;- dbConnect(\n  MariaDB(), host = \"scidb.smith.edu\",\n  user = \"waiuser\", password = \"smith_waiDB\", \n  dbname = \"wai\"\n)\n\n# Access the relavent tables\nMeasurements &lt;- tbl(con_wai, \"Measurements\")\nPI_Info &lt;- tbl(con_wai, \"PI_Info\")\nSubjects &lt;- tbl(con_wai, \"Subjects\")\n\nA look at all tables in the data set\n\nSHOW TABLES;\n\n\n7 records\n\n\nTables_in_wai\n\n\n\n\nCodebook\n\n\nMeasurements\n\n\nMeasurements_pre2020\n\n\nPI_Info\n\n\nPI_Info_OLD\n\n\nSubjects\n\n\nSubjects_pre2020\n\n\n\n\n\nView the Measurements variables\n\nDESCRIBE Measurements;\n\n\nDisplaying records 1 - 10\n\n\nField\nType\nNull\nKey\nDefault\nExtra\n\n\n\n\nIdentifier\nvarchar(50)\nNO\nPRI\nNA\n\n\n\nSubjectNumber\nint\nNO\nPRI\nNA\n\n\n\nSession\nint\nNO\nPRI\nNA\n\n\n\nEar\nvarchar(50)\nNO\nPRI\n\n\n\n\nInstrument\nvarchar(50)\nNO\nPRI\n\n\n\n\nAge\nfloat\nYES\n\nNA\n\n\n\nAgeCategory\nvarchar(50)\nYES\n\nNA\n\n\n\nEarStatus\nvarchar(50)\nYES\n\nNA\n\n\n\nTPP\nfloat\nYES\n\nNA\n\n\n\nAreaCanal\nfloat\nYES\n\nNA\n\n\n\n\n\n\nView the PI_Info variables\n\nDESCRIBE PI_Info;\n\n\nDisplaying records 1 - 10\n\n\nField\nType\nNull\nKey\nDefault\nExtra\n\n\n\n\nIdentifier\nvarchar(50)\nNO\nPRI\nNA\n\n\n\nYear\nint\nNO\n\nNA\n\n\n\nAuthors\ntext\nNO\n\nNA\n\n\n\nAuthorsShortList\ntext\nNO\n\nNA\n\n\n\nTitle\ntext\nNO\n\nNA\n\n\n\nJournal\ntext\nNO\n\nNA\n\n\n\nURL\ntext\nNO\n\nNA\n\n\n\nAbstract\ntext\nNO\n\nNA\n\n\n\nDataSubmitterName\ntext\nNO\n\nNA\n\n\n\nDataSubmitterEmail\ntext\nNO\n\nNA\n\n\n\n\n\n\nSelecting and viewing relevant columns within a study\n\nSELECT \n  Identifier,\n  Frequency,\n  Absorbance\nFROM Measurements\nWhere Identifier = \"Abur_2014\"\n\nLIMIT 0, 5;\n\n\n5 records\n\n\nIdentifier\nFrequency\nAbsorbance\n\n\n\n\nAbur_2014\n210.938\n0.0333379\n\n\nAbur_2014\n234.375\n0.0315705\n\n\nAbur_2014\n257.812\n0.0405751\n\n\nAbur_2014\n281.250\n0.0438399\n\n\nAbur_2014\n304.688\n0.0486400\n\n\n\n\n\n\nUsed the following queary to create an output variable called “data” to be used as the main data source for our plot in R.\n\n\nSELECT \n  Measurements.Identifier,\n  PI_Info.AuthorsShortList,\n  Measurements.Instrument,\n  Measurements.Frequency,\n  AVG(Measurements.Absorbance) AS MeanAbsorbance,\n  CONCAT(PI_Info.AuthorsShortList, ' et al. N=', \n         COUNT(DISTINCT CONCAT(Measurements.SubjectNumber, Measurements.Ear)), ', ', Measurements.Instrument) AS Legend_Label\nFROM Measurements\nJOIN PI_Info ON Measurements.Identifier = PI_Info.Identifier\nWHERE Measurements.Identifier IN ('Abur_2014', 'Feeney_207', 'Groon_2015', 'Lewis_2015', 'Liu_2008', 'Rosowski_2012', 'Shahnaz_2006', 'Shaver_2013', 'Sun_2016', 'Voss_1994', 'Voss_2010', 'Werner_2010')\n  AND Measurements.Frequency &gt;= 200  -- Apply frequency filter in SQL\nGROUP BY Measurements.Identifier, Measurements.Instrument, PI_Info.AuthorsShortList, Measurements.Frequency;\n\nThe resulting dataset was visualized in R to replicate the mean absorbance plot from Voss (2020):\n\nggplot(data, aes(x = Frequency, y = MeanAbsorbance, color = Legend_Label)) +\n  geom_line(size = 0.8) +\n  labs(\n    title = \"Mean Absorbance from Publications in WAI Database\",\n    x = \"Frequency (Hz)\",\n    y = \"Mean Absorbance\"\n  ) +\n  theme_minimal() +\n  scale_x_continuous(\n    trans = \"log10\",\n    breaks = c(200, 400, 600, 800, 1000, 2000, 4000, 6000, 8000),\n    labels = c(\"200\", \"400\", \"600\", \"800\", \"1000\", \"2000\", \"4000\", \"6000\", \"8000\"),\n    limits = c(200, 8000)\n  ) +\n  scale_y_continuous(\n    limits = c(0, 1),\n    expand = c(0, 0)\n  ) +\n  theme(\n    legend.position = c(0.2, 0.8),  # Place legend inside the plot\n    legend.title = element_blank()  # Remove legend title\n  )\n\n\n\n\n\n\n\n\nDescription:\nThis plot shows mean absorbance values across frequencies for the 12 studies in the WAI database. Each line represents a study, with a legend indicating the authors, the number of unique ears included, and the instrument used.\n\nShows the age categories\n\n\nSELECT DISTINCT AgeCategoryFirstMeasurement\nFROM Subjects;\n\n\n4 records\n\n\nAgeCategoryFirstMeasurement\n\n\n\n\nAdult\n\n\nInfant\n\n\nChild\n\n\nNICU\n\n\n\n\n\n\n\nStudy 1: Abur_2014\nI began by analyzing the Abur_2014 study for differences in absorbance by age category. I used the follwoing queary to create an output variable for this study called “age_data_Abur_2014”\n\nSELECT \n  Subjects.AgeCategoryFirstMeasurement AS AgeCategory, \n  Measurements.Frequency, \n  AVG(Measurements.Absorbance) AS MeanAbsorbance\nFROM Measurements\nJOIN Subjects ON Measurements.SubjectNumber = Subjects.SubjectNumber\nWHERE Measurements.Identifier = 'Abur_2014' \n  AND Measurements.Frequency &gt;= 200\nGROUP BY Subjects.AgeCategoryFirstMeasurement, Measurements.Frequency;\n\nRespective graph for Abur_2014 age data.\n\nggplot(age_data_Abur_2014, aes(x = Frequency, y = MeanAbsorbance, color = AgeCategory)) +\n  geom_line(size = 1) +\n  labs(\n    title = \"Frequency vs. Mean Absorbance by Age Category (Abur_2014)\",\n    x = \"Frequency (Hz)\",\n    y = \"Mean Absorbance\",\n    color = \"Age Category\"\n  ) +\n  theme_minimal() +\n  scale_x_continuous(\n    trans = \"log10\",\n    breaks = c(200, 400, 600, 800, 1000, 2000, 4000, 6000, 8000),\n    labels = c(\"200\", \"400\", \"600\", \"800\", \"1000\", \"2000\", \"4000\", \"6000\", \"8000\"),\n    limits = c(200, 8000)\n  ) +\n  scale_y_continuous(\n    limits = c(0, 1),\n    expand = c(0, 0)\n  ) +\n  theme(\n    legend.position = \"right\",\n    plot.title = element_text(hjust = 0.5)\n  )\n\n\n\n\n\n\n\n\nDescription:\nThis plot illustrates how mean absorbance varies across frequencies for different age categories in the Abur_2014 study. In this study all age groups were found to have nearly identical absorbance across frequencies.\nI found it very odd that the values for each age group are nearly identical. After talking with my professor we thought that perhabs this could have been due to a sample size issue. To check this I performed the following sql query:\n\nSELECT \n  Subjects.AgeCategoryFirstMeasurement AS AgeCategory, \n  COUNT(DISTINCT Measurements.SubjectNumber) AS SampleSize\nFROM Measurements\nJOIN Subjects ON Measurements.SubjectNumber = Subjects.SubjectNumber\nWHERE Measurements.Identifier = 'Abur_2014'\nGROUP BY Subjects.AgeCategoryFirstMeasurement;\n\n\n3 records\n\n\nAgeCategory\nSampleSize\n\n\n\n\nAdult\n7\n\n\nChild\n7\n\n\nInfant\n7\n\n\n\n\n\nHere we can see that there were only 7 observations in each age group category. Such a limited sample size may have restricted the ability to observe meaningful differences between age groupls.\n\nCheck which study included NICU in order to visuilze all age group catagories and view an additional study.\n\nSELECT *\nFROM Subjects\nWHERE AgeCategoryFirstMeasurement = 'NICU'\nLIMIT 100;\n\n\nDisplaying records 1 - 10\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIdentifier\nSubjectNumber\nSessionTotal\nAgeFirstMeasurement\nAgeCategoryFirstMeasurement\nSex\nRace\nEthnicity\nLeftEarStatusFirstMeasurement\nRightEarStatusFirstMeasurement\nSubjectNotes\n\n\n\n\nHunter_2016\n1005\n1\n1.0750000\nNICU\nMale\nCaucasian\nNonHispanic\nNormal\nNormal\nnone\n\n\nHunter_2016\n1026\n5\n0.0000000\nNICU\nFemale\nCaucasian\nNonHispanic\nNormal\nNormal\nnone\n\n\nHunter_2016\n1035\n4\n0.1916670\nNICU\nMale\nCaucasian\nNonHispanic\nNormal\nNormal\nnone\n\n\nHunter_2016\n1046\n2\n0.1500000\nNICU\nFemale\nCaucasian\nNonHispanic\nNormal\nNormal\nnone\n\n\nHunter_2016\n2001\n1\n0.9666670\nNICU\nMale\nCaucasian\nNonHispanic\nNormal\nNormal\nnone\n\n\nHunter_2016\n2006\n2\n0.8083330\nNICU\nMale\nCaucasian\nNonHispanic\nNormal\nNormal\nnone\n\n\nHunter_2016\n2008\n1\n0.8500000\nNICU\nFemale\nCaucasian\nNonHispanic\nNormal\nNormal\nnone\n\n\nHunter_2016\n2014\n2\n0.6500000\nNICU\nFemale\nBlack\nNonHispanic\nNormal\nNormal\nnone\n\n\nHunter_2016\n2030\n2\n0.1666670\nNICU\nFemale\nCaucasian\nNonHispanic\nNormal\nNormal\nnone\n\n\nHunter_2016\n2031\n2\n0.0166667\nNICU\nFemale\nCaucasian\nNonHispanic\nNormal\nNormal\nnone\n\n\n\n\n\n\n\nStudy 2: Hunter_2016 (Including NICU)\nNext, I explored the Hunter_2016 study, which includes NICU participants:\n\nSELECT \n  Subjects.AgeCategoryFirstMeasurement AS AgeCategory, \n  Measurements.Frequency, \n  AVG(Measurements.Absorbance) AS MeanAbsorbance\nFROM Measurements\nJOIN Subjects ON Measurements.SubjectNumber = Subjects.SubjectNumber\nWHERE Measurements.Identifier = 'Hunter_2016' \n  AND Measurements.Frequency &gt;= 200  -- Apply frequency filter in SQL\nGROUP BY Subjects.AgeCategoryFirstMeasurement, Measurements.Frequency;\n\nRespective graph for Hunter_2016 age data.\n\nggplot(age_data_Hunter_2016, aes(x = Frequency, y = MeanAbsorbance, color = AgeCategory)) +\n  geom_line(size = 1) +\n  labs(\n    title = \"Frequency vs. Mean Absorbance by Age Category (Hunter_2016)\",\n    x = \"Frequency (Hz)\",\n    y = \"Mean Absorbance\",\n    color = \"Age Category\"\n  ) +\n  theme_minimal() +\n  scale_x_continuous(\n    trans = \"log10\",\n    breaks = c(200, 400, 600, 800, 1000, 2000, 4000, 6000, 8000),\n    labels = c(\"200\", \"400\", \"600\", \"800\", \"1000\", \"2000\", \"4000\", \"6000\", \"8000\"),\n    limits = c(200, 8000)\n  ) +\n  scale_y_continuous(\n    limits = c(0, 1),\n    expand = c(0, 0)\n  ) +\n  theme(\n    legend.position = \"right\",\n    plot.title = element_text(hjust = 0.5)\n  )\n\n\n\n\n\n\n\n\nDescription:\nThis plot highlights the variation in absorbance across frequencies for the Hunter_2016 study. The NICU age group is included, and their absorbance patterns are compared with other age categories. In this study it was found that adults had significantly less absorbance im comparison to younger age groups.\n\n\n\nConclusion\nThis analysis successfully replicated Figure 1 from Voss (2020) by querying the WAI database and visualizing mean absorbance trends across studies. It also explored absorbance differences by age category for two studies, providing insight into how absorbance varies for different populations, including NICU. The project demonstrates the power of SQL for efficient data wrangling and R for effective visualization."
  },
  {
    "objectID": "Project4_SQL.html#introduction",
    "href": "Project4_SQL.html#introduction",
    "title": "Project 4 - SQL",
    "section": "",
    "text": "There are two main goals of this project. First, to recreate Figure 1 from Voss (2020) using SQL to query the Wideband Acoustic Immittance (WAI) Database. This requires calculating mean absorbance values for 12 selected studies and producing a correctly labeled plot in R. Second, to explore a study in the database where subjects of varying age categories, were enrolled. For this study, I will analyze frequency vs. mean absorption by age category and create an informative plot to visualize the data.\nThe SQL queries handle data filtering, aggregation, and JOIN operations to generate datasets ready for visualization in R. The analysis aims to demonstrate proficiency in both SQL for data wrangling and R for plotting.\nEstablish connection to the database and also create variables for needed tables for ease of use.\n\n#Establish Connection\nlibrary(RMariaDB)\nlibrary(tidyverse)\ncon_wai &lt;- dbConnect(\n  MariaDB(), host = \"scidb.smith.edu\",\n  user = \"waiuser\", password = \"smith_waiDB\", \n  dbname = \"wai\"\n)\n\n# Access the relavent tables\nMeasurements &lt;- tbl(con_wai, \"Measurements\")\nPI_Info &lt;- tbl(con_wai, \"PI_Info\")\nSubjects &lt;- tbl(con_wai, \"Subjects\")\n\nA look at all tables in the data set\n\nSHOW TABLES;\n\n\n7 records\n\n\nTables_in_wai\n\n\n\n\nCodebook\n\n\nMeasurements\n\n\nMeasurements_pre2020\n\n\nPI_Info\n\n\nPI_Info_OLD\n\n\nSubjects\n\n\nSubjects_pre2020\n\n\n\n\n\nView the Measurements variables\n\nDESCRIBE Measurements;\n\n\nDisplaying records 1 - 10\n\n\nField\nType\nNull\nKey\nDefault\nExtra\n\n\n\n\nIdentifier\nvarchar(50)\nNO\nPRI\nNA\n\n\n\nSubjectNumber\nint\nNO\nPRI\nNA\n\n\n\nSession\nint\nNO\nPRI\nNA\n\n\n\nEar\nvarchar(50)\nNO\nPRI\n\n\n\n\nInstrument\nvarchar(50)\nNO\nPRI\n\n\n\n\nAge\nfloat\nYES\n\nNA\n\n\n\nAgeCategory\nvarchar(50)\nYES\n\nNA\n\n\n\nEarStatus\nvarchar(50)\nYES\n\nNA\n\n\n\nTPP\nfloat\nYES\n\nNA\n\n\n\nAreaCanal\nfloat\nYES\n\nNA\n\n\n\n\n\n\nView the PI_Info variables\n\nDESCRIBE PI_Info;\n\n\nDisplaying records 1 - 10\n\n\nField\nType\nNull\nKey\nDefault\nExtra\n\n\n\n\nIdentifier\nvarchar(50)\nNO\nPRI\nNA\n\n\n\nYear\nint\nNO\n\nNA\n\n\n\nAuthors\ntext\nNO\n\nNA\n\n\n\nAuthorsShortList\ntext\nNO\n\nNA\n\n\n\nTitle\ntext\nNO\n\nNA\n\n\n\nJournal\ntext\nNO\n\nNA\n\n\n\nURL\ntext\nNO\n\nNA\n\n\n\nAbstract\ntext\nNO\n\nNA\n\n\n\nDataSubmitterName\ntext\nNO\n\nNA\n\n\n\nDataSubmitterEmail\ntext\nNO\n\nNA\n\n\n\n\n\n\nSelecting and viewing relevant columns within a study\n\nSELECT \n  Identifier,\n  Frequency,\n  Absorbance\nFROM Measurements\nWhere Identifier = \"Abur_2014\"\n\nLIMIT 0, 5;\n\n\n5 records\n\n\nIdentifier\nFrequency\nAbsorbance\n\n\n\n\nAbur_2014\n210.938\n0.0333379\n\n\nAbur_2014\n234.375\n0.0315705\n\n\nAbur_2014\n257.812\n0.0405751\n\n\nAbur_2014\n281.250\n0.0438399\n\n\nAbur_2014\n304.688\n0.0486400\n\n\n\n\n\nThe following query calculates mean absorbance values for the 12 studies in Voss (2020), along with data for creating a descriptive plot legend:\n\n\nSELECT \n  Measurements.Identifier,\n  PI_Info.AuthorsShortList,\n  Measurements.Instrument,\n  Measurements.Frequency,\n  AVG(Measurements.Absorbance) AS MeanAbsorbance,\n  CONCAT(PI_Info.AuthorsShortList, ' et al. N=', \n         COUNT(DISTINCT CONCAT(Measurements.SubjectNumber, Measurements.Ear)), ', ', Measurements.Instrument) AS Legend_Label\nFROM Measurements\nJOIN PI_Info ON Measurements.Identifier = PI_Info.Identifier\nWHERE Measurements.Identifier IN ('Abur_2014', 'Feeney_207', 'Groon_2015', 'Lewis_2015', 'Liu_2008', 'Rosowski_2012', 'Shahnaz_2006', 'Shaver_2013', 'Sun_2016', 'Voss_1994', 'Voss_2010', 'Werner_2010')\nGROUP BY Measurements.Identifier, Measurements.Instrument, PI_Info.AuthorsShortList, Measurements.Frequency;\n\n\nDisplaying records 1 - 10\n\n\n\n\n\n\n\n\n\n\nIdentifier\nAuthorsShortList\nInstrument\nFrequency\nMeanAbsorbance\nLegend_Label\n\n\n\n\nAbur_2014\nAbur et al.\nHearID\n210.938\n0.0784746\nAbur et al. et al. N=14, HearID\n\n\nAbur_2014\nAbur et al.\nHearID\n234.375\n0.0826420\nAbur et al. et al. N=14, HearID\n\n\nAbur_2014\nAbur et al.\nHearID\n257.812\n0.0948482\nAbur et al. et al. N=14, HearID\n\n\nAbur_2014\nAbur et al.\nHearID\n281.250\n0.1031472\nAbur et al. et al. N=14, HearID\n\n\nAbur_2014\nAbur et al.\nHearID\n304.688\n0.1137576\nAbur et al. et al. N=14, HearID\n\n\nAbur_2014\nAbur et al.\nHearID\n328.125\n0.1221205\nAbur et al. et al. N=14, HearID\n\n\nAbur_2014\nAbur et al.\nHearID\n351.562\n0.1334329\nAbur et al. et al. N=14, HearID\n\n\nAbur_2014\nAbur et al.\nHearID\n375.000\n0.1447725\nAbur et al. et al. N=14, HearID\n\n\nAbur_2014\nAbur et al.\nHearID\n398.438\n0.1563874\nAbur et al. et al. N=14, HearID\n\n\nAbur_2014\nAbur et al.\nHearID\n421.875\n0.1806973\nAbur et al. et al. N=14, HearID\n\n\n\n\n\nTurns the previous query into an output variable called “data” to be used as the main data source for our plot in R.\n\n\nSELECT \n  Measurements.Identifier,\n  PI_Info.AuthorsShortList,\n  Measurements.Instrument,\n  Measurements.Frequency,\n  AVG(Measurements.Absorbance) AS MeanAbsorbance,\n  CONCAT(PI_Info.AuthorsShortList, ' et al. N=', \n         COUNT(DISTINCT CONCAT(Measurements.SubjectNumber, Measurements.Ear)), ', ', Measurements.Instrument) AS Legend_Label\nFROM Measurements\nJOIN PI_Info ON Measurements.Identifier = PI_Info.Identifier\nWHERE Measurements.Identifier IN ('Abur_2014', 'Feeney_207', 'Groon_2015', 'Lewis_2015', 'Liu_2008', 'Rosowski_2012', 'Shahnaz_2006', 'Shaver_2013', 'Sun_2016', 'Voss_1994', 'Voss_2010', 'Werner_2010')\nGROUP BY Measurements.Identifier, Measurements.Instrument, PI_Info.AuthorsShortList, Measurements.Frequency;\n\nThe resulting dataset was visualized in R to replicate the mean absorbance plot from Voss (2020):\n\ndata$Frequency &lt;- as.numeric(data$Frequency)\ndata &lt;- data |&gt;\n  filter(Frequency &gt;= 200)\n\nggplot(data, aes(x = Frequency, y = MeanAbsorbance, color = Legend_Label)) +\n  geom_line(size = 0.8) +\n  labs(\n    title = \"Mean absorbance from publication's in WAI database\",\n    x = \"Frequency (Hz)\",\n    y = \"Mean Absorbance\",\n  ) +\n  theme_minimal() +\n  scale_x_continuous(\n    trans = \"log10\",\n    breaks = c(200, 400, 600, 800, 1000, 2000, 4000, 6000, 8000),\n    labels = c(\"200\", \"400\", \"600\", \"800\", \"1000\", \"2000\", \"4000\", \"6000\", \"8000\"),\n    limits = c(200, 8000)\n  ) +\n  scale_y_continuous(\n    limits = c(0, 1),\n    expand = c(0, 0)\n  )"
  },
  {
    "objectID": "DS_Presentation.html#visulization-using-line-graph-from-ggplot",
    "href": "DS_Presentation.html#visulization-using-line-graph-from-ggplot",
    "title": "Data Science Final Presentation",
    "section": "Visulization Using Line Graph from ggplot",
    "text": "Visulization Using Line Graph from ggplot\n\n\n# A tibble: 5 × 2\n  month      avg_weight\n  &lt;date&gt;          &lt;dbl&gt;\n1 2020-06-01       78.8\n2 2020-07-01      219. \n3 2020-08-01      473. \n4 2020-09-01     1128. \n5 2020-10-01      959."
  },
  {
    "objectID": "DS_Presentation.html#project-1---part-1---lisas-vegetable-garden",
    "href": "DS_Presentation.html#project-1---part-1---lisas-vegetable-garden",
    "title": "Data Science Final Presentation",
    "section": "Project 1 - Part 1 - Lisa’s Vegetable Garden",
    "text": "Project 1 - Part 1 - Lisa’s Vegetable Garden\nThis project was my first experience collecting, cleaning, and visulizing data.\nFor this analysis I selected the “Lisa’s Vegetable Garden” dataset from tidytuesday as it sounded funny and interesting.\nThe data contained information on Lisa Lendway’s vegetable garden from the summers of 2020 and 2021.\nIt includes planting, harvesting, and spending details between the two years.\nI chose to visualize the average weight of vegetables harvested per month in the 2020 harvest season."
  },
  {
    "objectID": "DS_Presentation.html#project-2---part-2---carbon-emissions",
    "href": "DS_Presentation.html#project-2---part-2---carbon-emissions",
    "title": "Data Science Final Presentation",
    "section": "Project 2 - Part 2 - Carbon Emissions",
    "text": "Project 2 - Part 2 - Carbon Emissions\nFor this analysis I chose another interesting tidytuesday data set called “Carbon Emissions”.\nThis data set contained information on historical emissions data from 1854 to present.\nI wanted to visualize how carbon emissions have changed over time."
  },
  {
    "objectID": "DS_Presentation.html#visulization",
    "href": "DS_Presentation.html#visulization",
    "title": "Data Science Final Presentation",
    "section": "Visulization",
    "text": "Visulization\n\n\n# A tibble: 5 × 2\n  month      avg_weight\n  &lt;date&gt;          &lt;dbl&gt;\n1 2020-06-01       78.8\n2 2020-07-01      219. \n3 2020-08-01      473. \n4 2020-09-01     1128. \n5 2020-10-01      959."
  },
  {
    "objectID": "DS_Presentation.html#visulization-1",
    "href": "DS_Presentation.html#visulization-1",
    "title": "Data Science Final Presentation",
    "section": "Visulization",
    "text": "Visulization\n\nsummary_data &lt;- emissions |&gt;\n  select(year, total_emissions_MtCO2e) |&gt;\n  group_by(year) |&gt;\n  summarize(ave_emissions = mean(total_emissions_MtCO2e, na.rm = TRUE))"
  },
  {
    "objectID": "DS_Presentation.html#project-1---part-1-lisas-vegetable-garden",
    "href": "DS_Presentation.html#project-1---part-1-lisas-vegetable-garden",
    "title": "Data Science Final Presentation",
    "section": "Project 1 - Part 1: Lisa’s Vegetable Garden",
    "text": "Project 1 - Part 1: Lisa’s Vegetable Garden\n\nDataset: TidyTuesday’s May 28, 2024 release.\n\nRecords from Lisa Lendway’s 2020 and 2021 vegetable gardens.\nIncludes planting, harvesting, and spending data.\n\nPlan: Explore harvest trends in 2020 by calculating average monthly harvest weights and visualizing seasonal patterns.\nFindings:\n\nHarvest weights increased steadily from June, peaked in September, and declined slightly in October.\nDeveloped skills in data wrangling, date manipulation, and visualization with the tidyverse."
  },
  {
    "objectID": "DS_Presentation.html#project-2---part-2-carbon-emissions",
    "href": "DS_Presentation.html#project-2---part-2-carbon-emissions",
    "title": "Data Science Final Presentation",
    "section": "Project 2 - Part 2: Carbon Emissions",
    "text": "Project 2 - Part 2: Carbon Emissions\n\nDataset: TidyTuesday’s May 21, 2024 release.\n\nHistorical emissions data from 1854–present.\n\nPlan: Summarize average annual carbon emissions and visualize trends over time.\nFindings:\n\nEmissions rose sharply during the mid-20th century, reflecting industrialization and increased energy use.\nStrengthened skills in data summarization, trend analysis, and visualization using the tidyverse."
  },
  {
    "objectID": "DS_Presentation.html#project-2-netflix-title-analysis",
    "href": "DS_Presentation.html#project-2-netflix-title-analysis",
    "title": "Data Science Final Presentation",
    "section": "Project 2: Netflix Title Analysis",
    "text": "Project 2: Netflix Title Analysis\n\nDataset: TidyTuesday’s April 20, 2021 release.\n\nMetadata on Netflix movies and TV shows, including release years, titles, and associated details.\n\nPlan: Explore trends in Netflix’s content catalog:\n\nAnalyze the distribution of movies vs. TV shows.\nExamine release year trends and common title keywords.\n\nFindings:\n\nNetflix’s library is dominated by movies and modern content (post-2000).\nLearned to apply string manipulation and visualization techniques with tidyverse."
  },
  {
    "objectID": "DS_Presentation.html#visualization-1",
    "href": "DS_Presentation.html#visualization-1",
    "title": "Data Science Final Presentation",
    "section": "Visualization 1",
    "text": "Visualization 1\n\nnetflix_type_count &lt;- netflix |&gt;\n  count(type)"
  },
  {
    "objectID": "DS_Presentation.html#visualization-2",
    "href": "DS_Presentation.html#visualization-2",
    "title": "Data Science Final Presentation",
    "section": "Visualization 2",
    "text": "Visualization 2\n\nnetflix_year_count &lt;- netflix |&gt;\n  count(release_year) |&gt;\n  arrange(desc(n))"
  },
  {
    "objectID": "DS_Presentation.html#visualization-3",
    "href": "DS_Presentation.html#visualization-3",
    "title": "Data Science Final Presentation",
    "section": "Visualization 3",
    "text": "Visualization 3"
  },
  {
    "objectID": "DS_Presentation.html#project-3-ncaa-home-court-advantage",
    "href": "DS_Presentation.html#project-3-ncaa-home-court-advantage",
    "title": "Data Science Final Presentation",
    "section": "Project 3: NCAA Home-Court Advantage",
    "text": "Project 3: NCAA Home-Court Advantage\n\nDataset: Kaggle March Madness Data (2008–2024, excluding 2020).\nPlan: Test the null hypothesis of no difference in win percentages between home and away games using permutation testing.\nFindings:\n\nObserved a 31% higher win percentage at home.\nLearned to simulate random outcomes, perform hypothesis testing, and visualize results."
  },
  {
    "objectID": "DS_Presentation.html#permutation-and-null-hypothesis-test",
    "href": "DS_Presentation.html#permutation-and-null-hypothesis-test",
    "title": "Data Science Final Presentation",
    "section": "Permutation and Null Hypothesis Test",
    "text": "Permutation and Null Hypothesis Test\n\n#Calculate the Observed Difference in Median Win Percentage\nobserved_diff &lt;- combined_median_data |&gt;\n  group_by(location) |&gt;\n  summarise(median_win_pct = median(median_win_pct, na.rm = TRUE)) |&gt;\n  summarise(diff = diff(median_win_pct)) |&gt;\n  pull(diff)\n\ncat(\"Observed Difference in Median Win Percentage (Home - Away):\", observed_diff, \"\\n\")\n\nObserved Difference in Median Win Percentage (Home - Away): 31.16884 \n\n\n\n# Define Permutation Function (Shuffling location within each team)\ncalculate_permutation &lt;- function(data) {\n  data |&gt;\n    group_by(TEAM) |&gt;\n    mutate(location = sample(location, replace = FALSE)) |&gt;\n    group_by(location) |&gt;\n    summarise(median_win_pct = median(median_win_pct, na.rm = TRUE)) |&gt;\n    summarise(diff = diff(median_win_pct)) |&gt;\n    pull(diff)\n}\n#Permutation Test\nnum_permutations &lt;- 10000\nperm_results &lt;- map_dbl(1:num_permutations, ~ calculate_permutation(combined_median_data))\n\n# Calculate the two-sided p-value\np_value &lt;- mean(abs(perm_results) &gt;= abs(observed_diff))\ncat(\"Two-Sided P-value:\", p_value, \"\\n\")\n\nTwo-Sided P-value: 0"
  },
  {
    "objectID": "DS_Presentation.html#visulization-2",
    "href": "DS_Presentation.html#visulization-2",
    "title": "Data Science Final Presentation",
    "section": "Visulization",
    "text": "Visulization"
  },
  {
    "objectID": "DS_Presentation.html#project-4-sql-analysis-of-wai-auditory-data",
    "href": "DS_Presentation.html#project-4-sql-analysis-of-wai-auditory-data",
    "title": "Data Science Final Presentation",
    "section": "Project 4: SQL Analysis of WAI Auditory Data",
    "text": "Project 4: SQL Analysis of WAI Auditory Data\n\nGoals: Recreate Figure 1 from Voss (2019) and analyze absorbance trends by age.\nApproach: Use SQL to query the WAI Database and R for visualization.\nFindings:\n\nReplicated Voss (2019) Figure 1 for 12 studies.\nFound age-related differences in Hunter (2016), including NICU data."
  },
  {
    "objectID": "DS_Presentation.html#visulization-1-recreating-voss-graph",
    "href": "DS_Presentation.html#visulization-1-recreating-voss-graph",
    "title": "Data Science Final Presentation",
    "section": "Visulization 1: Recreating Voss Graph",
    "text": "Visulization 1: Recreating Voss Graph\n\n\nSELECT \n  Measurements.Identifier,\n  PI_Info.AuthorsShortList,\n  Measurements.Instrument,\n  Measurements.Frequency,\n  AVG(Measurements.Absorbance) AS MeanAbsorbance,\n  CONCAT(PI_Info.AuthorsShortList, ' et al. N=', \n         COUNT(DISTINCT CONCAT(Measurements.SubjectNumber, Measurements.Ear)), ', ', Measurements.Instrument) AS Legend_Label\nFROM Measurements\nJOIN PI_Info ON Measurements.Identifier = PI_Info.Identifier\nWHERE Measurements.Identifier IN ('Abur_2014', 'Feeney_207', 'Groon_2015', 'Lewis_2015', 'Liu_2008', 'Rosowski_2012', 'Shahnaz_2006', 'Shaver_2013', 'Sun_2016', 'Voss_1994', 'Voss_2010', 'Werner_2010')\n  AND Measurements.Frequency &gt;= 200  -- Apply frequency filter in SQL\nGROUP BY Measurements.Identifier, Measurements.Instrument, PI_Info.AuthorsShortList, Measurements.Frequency;"
  },
  {
    "objectID": "DS_Presentation.html#visulization-2-deeper-look-into-hunter-2016-study",
    "href": "DS_Presentation.html#visulization-2-deeper-look-into-hunter-2016-study",
    "title": "Data Science Final Presentation",
    "section": "Visulization 2: Deeper look into Hunter 2016 Study",
    "text": "Visulization 2: Deeper look into Hunter 2016 Study\n\nSELECT \n  Subjects.AgeCategoryFirstMeasurement AS AgeCategory, \n  Measurements.Frequency, \n  AVG(Measurements.Absorbance) AS MeanAbsorbance\nFROM Measurements\nJOIN Subjects ON Measurements.SubjectNumber = Subjects.SubjectNumber\nWHERE Measurements.Identifier = 'Hunter_2016' \n  AND Measurements.Frequency &gt;= 200  -- Apply frequency filter in SQL\nGROUP BY Subjects.AgeCategoryFirstMeasurement, Measurements.Frequency;"
  },
  {
    "objectID": "DS_Presentation.html#thanks-for-listening",
    "href": "DS_Presentation.html#thanks-for-listening",
    "title": "Data Science Final Presentation",
    "section": "Thanks For Listening!",
    "text": "Thanks For Listening!"
  }
]