[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "I am a sophomore at Pomona College intending to double major in Economics and Math, while also pursuing a minor in Data Science. I am also on the Pomona College swim team. I have been working on my data analysis skills and learning R. I have worked on several TidyTuesday datasets to create visuals.\nCheck out the Data Viz tab on the top left to check out some interesting data visuals."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Ian’s Website",
    "section": "",
    "text": "Welcome to my website! Click the links down below for more information.\nBiography: I am a sophomore at Pomona College intending to double major in Economics and Math, while also pursuing a minor in Data Science. I am also on the Pomona College swim team. I am originally from the San Francisco East Bay Area."
  },
  {
    "objectID": "Lisa Vegetable garden.html",
    "href": "Lisa Vegetable garden.html",
    "title": "Lisa Vegetable Garden",
    "section": "",
    "text": "tuesdata &lt;- tidytuesdayR::tt_load(2024, week = 22)\n\nharvest_2020 &lt;- tuesdata$harvest_2020\nharvest_2021 &lt;- tuesdata$harvest_2021\nplanting_2020 &lt;- tuesdata$planting_2020\nplanting_2021 &lt;- tuesdata$planting_2021\nspending_2020 &lt;- tuesdata$spending_2020\nspending_2021 &lt;- tuesdata$spending_2021\n\nBelow you can see my code and data visualization of Lisa’s Vegetable Garden. The graph represents the average weight of vegetables harvested during each month of the 2020 harvest.\n\nlibrary(tidyverse)\n\n\nharvest_2020_avg &lt;- harvest_2020 |&gt;\n  mutate(month = as.Date(format(as.Date(date), \"%Y-%m-01\"))) |&gt;  # Extract month and convert to Date format\n  group_by(month) |&gt;\n  summarize(avg_weight = mean(weight, na.rm = TRUE))\n\nggplot(harvest_2020_avg, aes(x = month, y = avg_weight)) +\n  geom_line() +\n  geom_point() +\n  labs(title = \"Average Weight of Vegetables Harvested per Month in 2020 Season\",\n       x = \"Month\",\n       y = \"Average Weight (grams)\")"
  },
  {
    "objectID": "Carbon Emissions.html",
    "href": "Carbon Emissions.html",
    "title": "Carbon Emissions",
    "section": "",
    "text": "tuesdata &lt;- tidytuesdayR::tt_load(2024, week = 21)\n\nemissions &lt;- tuesdata$emissions\n\nBelow you can see my code and data visualization of Carbon Emissions overtime in America. The graph shows the increase in Carbon Emissions from 1850-Present Day.\n\nlibrary(tidyverse)\n\n\nsummary_data &lt;- emissions |&gt;\n  select(year, total_emissions_MtCO2e) |&gt;\n  group_by(year) |&gt;\n  summarize(ave_emissions = mean(total_emissions_MtCO2e))\n\nggplot(summary_data, aes(x = year, y = ave_emissions)) +\n  geom_point() +\n  geom_smooth(se = FALSE) +\n  labs(\n    x = \"Year\",\n    y = \"Average Carbon Emissions\",\n    title = \"Total Carbon Emissions per Year\"\n  )"
  },
  {
    "objectID": "Mini Project 2.html",
    "href": "Mini Project 2.html",
    "title": "Mini Project 2",
    "section": "",
    "text": "test here\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n\n\ntuesdata &lt;- tidytuesdayR::tt_load('2021-04-20')\ntuesdata &lt;- tidytuesdayR::tt_load(2021, week = 17)\n\nnetflix &lt;- tuesdata$netflix\n\n\nnetflix\n\n# A tibble: 7,787 × 12\n   show_id type    title director   cast  country date_added release_year rating\n   &lt;chr&gt;   &lt;chr&gt;   &lt;chr&gt; &lt;chr&gt;      &lt;chr&gt; &lt;chr&gt;   &lt;chr&gt;             &lt;dbl&gt; &lt;chr&gt; \n 1 s1      TV Show 3%    &lt;NA&gt;       João… Brazil  August 14…         2020 TV-MA \n 2 s2      Movie   7:19  Jorge Mic… Demi… Mexico  December …         2016 TV-MA \n 3 s3      Movie   23:59 Gilbert C… Tedd… Singap… December …         2011 R     \n 4 s4      Movie   9     Shane Ack… Elij… United… November …         2009 PG-13 \n 5 s5      Movie   21    Robert Lu… Jim … United… January 1…         2008 PG-13 \n 6 s6      TV Show 46    Serdar Ak… Erda… Turkey  July 1, 2…         2016 TV-MA \n 7 s7      Movie   122   Yasir Al … Amin… Egypt   June 1, 2…         2019 TV-MA \n 8 s8      Movie   187   Kevin Rey… Samu… United… November …         1997 R     \n 9 s9      Movie   706   Shravan K… Divy… India   April 1, …         2019 TV-14 \n10 s10     Movie   1920  Vikram Bh… Rajn… India   December …         2008 TV-MA \n# ℹ 7,777 more rows\n# ℹ 3 more variables: duration &lt;chr&gt;, listed_in &lt;chr&gt;, description &lt;chr&gt;\n\n\n\n# Distribution of content type\nnetflix_type_count &lt;- netflix |&gt;\n  summarize(n())\n\nnetflix_type_count\n\n# A tibble: 1 × 1\n  `n()`\n  &lt;int&gt;\n1  7787\n\n# Distribution of content release year\n#netflix_year_count &lt;- netflix |&gt;\n#  count(release_year)\n#netflix_year_"
  },
  {
    "objectID": "Mini Project 2 copy.html",
    "href": "Mini Project 2 copy.html",
    "title": "Mini Project 2",
    "section": "",
    "text": "This analysis uses data from the Netflix Titles Data Source, which is available through the TidyTuesday github repository.\n\nlibrary(tidyverse)\n\ntuesdata &lt;- tidytuesdayR::tt_load('2021-04-20')\n\nnetflix &lt;- tuesdata$netflix\n\n\nnetflix\n\n# A tibble: 7,787 × 12\n   show_id type    title director   cast  country date_added release_year rating\n   &lt;chr&gt;   &lt;chr&gt;   &lt;chr&gt; &lt;chr&gt;      &lt;chr&gt; &lt;chr&gt;   &lt;chr&gt;             &lt;dbl&gt; &lt;chr&gt; \n 1 s1      TV Show 3%    &lt;NA&gt;       João… Brazil  August 14…         2020 TV-MA \n 2 s2      Movie   7:19  Jorge Mic… Demi… Mexico  December …         2016 TV-MA \n 3 s3      Movie   23:59 Gilbert C… Tedd… Singap… December …         2011 R     \n 4 s4      Movie   9     Shane Ack… Elij… United… November …         2009 PG-13 \n 5 s5      Movie   21    Robert Lu… Jim … United… January 1…         2008 PG-13 \n 6 s6      TV Show 46    Serdar Ak… Erda… Turkey  July 1, 2…         2016 TV-MA \n 7 s7      Movie   122   Yasir Al … Amin… Egypt   June 1, 2…         2019 TV-MA \n 8 s8      Movie   187   Kevin Rey… Samu… United… November …         1997 R     \n 9 s9      Movie   706   Shravan K… Divy… India   April 1, …         2019 TV-14 \n10 s10     Movie   1920  Vikram Bh… Rajn… India   December …         2008 TV-MA \n# ℹ 7,777 more rows\n# ℹ 3 more variables: duration &lt;chr&gt;, listed_in &lt;chr&gt;, description &lt;chr&gt;\n\n\n\n# Distribution of content type\nnetflix_type_count &lt;- netflix |&gt;\n  count(type)\n\nnetflix_type_count\n\n# A tibble: 2 × 2\n  type        n\n  &lt;chr&gt;   &lt;int&gt;\n1 Movie    5377\n2 TV Show  2410\n\n# Distribution of content release year\nnetflix_year_count &lt;- netflix |&gt;\n  count(release_year) |&gt;\n  arrange(desc(n))\nnetflix_year_count\n\n# A tibble: 73 × 2\n   release_year     n\n          &lt;dbl&gt; &lt;int&gt;\n 1         2018  1121\n 2         2017  1012\n 3         2019   996\n 4         2016   882\n 5         2020   868\n 6         2015   541\n 7         2014   334\n 8         2013   267\n 9         2012   219\n10         2010   173\n# ℹ 63 more rows\n\n\nHere we can see data frames for the amount of movies and TV shows on Netflix from each release year and the amount of Movies and TV shows on Netflix. We can see that there are around double the amount of movies compared to TV shows. There seems to be a trend of more modern TV shows and movies on Netflix.\n\n# Plotting the distribution of content type\nggplot(netflix_type_count, aes(x = type, y = n, fill = type)) +\n  geom_bar(stat = \"identity\") +\n  labs(title = \"Distribution of Movies vs TV Shows\", x = \"Type\", y = \"Count\")\n\n\n\n\n\n\n\n# Plotting the distribution of Release year\nggplot(netflix_year_count, aes(x = release_year, y = n)) +\n  geom_line() +\n  labs(title = \"Netflix Content Release Year Distribution\", x = \"Release Year\", y = \"Count\")\n\n\n\n\n\n\n\n\nVisual represetations of the previous data frames.\n\n# Count titles containing the word \"love\"\nlove_count &lt;- netflix |&gt;\n  filter(str_detect(str_to_lower(title), \"love\")) |&gt;\n  summarise(count = n())\n\nlove_count\n\n# A tibble: 1 × 1\n  count\n  &lt;int&gt;\n1   175\n\n\nUsed str functions str_detect() and str_to_lower() to look for a popular word. Used regular expression “love” to look for the word love in movie and TV titles.\n\n# Titles containing the words \"life\", \"world\", or \"death\"\nkeywords_titles &lt;- netflix |&gt;\n  filter(str_detect(str_to_lower(title), \"life|world|death\"))\n\nkeywords_titles\n\n# A tibble: 179 × 12\n   show_id type    title   director cast  country date_added release_year rating\n   &lt;chr&gt;   &lt;chr&gt;   &lt;chr&gt;   &lt;chr&gt;    &lt;chr&gt; &lt;chr&gt;   &lt;chr&gt;             &lt;dbl&gt; &lt;chr&gt; \n 1 s150    Movie   A Beau… Andrew … Qi S… China,… December …         2011 TV-14 \n 2 s195    Movie   A Life… Francis… &lt;NA&gt;  Argent… March 20,…         2020 TV-14 \n 3 s310    Movie   Addict… Thierry… Anto… France  June 12, …         2014 TV-14 \n 4 s325    TV Show After … &lt;NA&gt;     Rick… United… April 24,…         2020 TV-MA \n 5 s379    Movie   Alex F… Alex Dí… Alex… Mexico  January 2…         2020 TV-MA \n 6 s399    TV Show Alien … &lt;NA&gt;     &lt;NA&gt;  United… December …         2020 TV-PG \n 7 s421    Movie   All th… Yibrán … Háns… Mexico  January 3…         2020 TV-14 \n 8 s530    Movie   Animal… Han Yan  Li Y… China   September…         2018 TV-MA \n 9 s544    TV Show Anothe… &lt;NA&gt;     Kate… United… July 25, …         2019 TV-MA \n10 s559    TV Show Apache… &lt;NA&gt;     Balt… Argent… August 16…         2019 TV-MA \n# ℹ 169 more rows\n# ℹ 3 more variables: duration &lt;chr&gt;, listed_in &lt;chr&gt;, description &lt;chr&gt;\n\n\nUses regular expression “life|world|death” to search for more possible popular words in move/tv show titles. Returns the entire data frame.\n\n# Count occurrences of each word \"life\", \"world\", and \"death\" in the titles\nword_count &lt;- netflix |&gt;\n  mutate(title_lower = str_to_lower(title)) |&gt;\n  \n  summarise(\n    life_count = sum(str_count(title_lower, \"life\")),\n    world_count = sum(str_count(title_lower, \"world\")),\n    death_count = sum(str_count(title_lower, \"death\"))\n  )\n\n\nword_count\n\n# A tibble: 1 × 3\n  life_count world_count death_count\n       &lt;int&gt;       &lt;int&gt;       &lt;int&gt;\n1         79          77          24\n\n\nGives a count for each of the selected words.\n\n# Count the number of occurrences of each director, excluding missing values\npopular_directors &lt;- netflix |&gt;\n  filter(!is.na(director) & director != \"\") |&gt;\n  count(director) |&gt;\n  arrange(desc(n))\n\n# View the top 10 most popular directors\nhead(popular_directors, 10)\n\n# A tibble: 10 × 2\n   director                   n\n   &lt;chr&gt;                  &lt;int&gt;\n 1 Raúl Campos, Jan Suter    18\n 2 Marcus Raboy              16\n 3 Jay Karas                 14\n 4 Cathy Garcia-Molina       13\n 5 Jay Chapman               12\n 6 Martin Scorsese           12\n 7 Youssef Chahine           12\n 8 Steven Spielberg          10\n 9 David Dhawan               9\n10 Hakan Algül                8\n\n# Separate the cast into individual actors and count their occurrences\npopular_actors &lt;- netflix |&gt;\n  filter(!is.na(cast) & cast != \"\") |&gt;\n  separate_rows(cast, sep = \", \") |&gt;\n  count(cast) |&gt;\n  arrange(desc(n))\n\n# View the top 10 most popular actors\nhead(popular_actors, 10)\n\n# A tibble: 10 × 2\n   cast                 n\n   &lt;chr&gt;            &lt;int&gt;\n 1 Anupam Kher         42\n 2 Shah Rukh Khan      35\n 3 Naseeruddin Shah    30\n 4 Om Puri             30\n 5 Akshay Kumar        29\n 6 Takahiro Sakurai    29\n 7 Amitabh Bachchan    27\n 8 Boman Irani         27\n 9 Paresh Rawal        27\n10 Yuki Kaji           27\n\n# Separate the country column into individual countries and count their occurrences\npopular_countries &lt;- netflix |&gt;\n  filter(!is.na(country) & country != \"\") |&gt;\n  separate_rows(country, sep = \", \") |&gt;\n  count(country) |&gt;\n  arrange(desc(n))\n\n# View the top 10 most popular countries\nhead(popular_countries, 10)\n\n# A tibble: 10 × 2\n   country            n\n   &lt;chr&gt;          &lt;int&gt;\n 1 United States   3296\n 2 India            990\n 3 United Kingdom   722\n 4 Canada           412\n 5 France           349\n 6 Japan            287\n 7 Spain            215\n 8 South Korea      212\n 9 Germany          199\n10 Mexico           154\n\n\nThis code performs three different tasks of counting occurrences of directors, actors, and countries, and then displaying the top 10 most popular in each category. It filters out missing or empty values and splits data where necessary (such as for actors and countries with multiple entries separated by commas).\nIt was very interesting to see the large amount of Indian actors appearing in tv shows/movies compared to the total amount of Indian movies. Perhaps there is a fewer supply of popular actors in Bollywood compared to Hollywood in the United States.\n\nnetflix_lowercase_titles &lt;- netflix |&gt;\n  mutate(lower_title = str_to_lower(title))\n\n#Use regular expressions to extract words and exclude those containing digits\nnetflix_words &lt;- netflix_lowercase_titles |&gt;\n  separate_rows(lower_title, sep = \"\\\\s+\") |&gt;\n  filter(!str_detect(lower_title, \"\\\\d\"))\n\n#Remove common words that are not of interest\ncommon_words &lt;- c(\"the\", \"and\", \"in\", \"of\", \"to\", \"a\", \"is\", \"for\", \"with\", \"on\", \"at\", \"by\", \"an\", \"from\", \"i\", \"&\")\n\nnetflix_words_final &lt;- netflix_words |&gt;\n  filter(!is.element(lower_title, common_words))\n\n#Count the frequency of each word\npopular_words &lt;- netflix_words_final |&gt;\n  count(lower_title, sort = TRUE)\n\nhead(popular_words, 10)\n\n# A tibble: 10 × 2\n   lower_title     n\n   &lt;chr&gt;       &lt;int&gt;\n 1 love          134\n 2 my            127\n 3 christmas      77\n 4 man            71\n 5 you            70\n 6 story          65\n 7 life           59\n 8 world          59\n 9 little         58\n10 one            53\n\n\n\npopular_words |&gt;\n  head(10) |&gt;\n  ggplot(aes(x = reorder(lower_title, n), y = n)) +\n  geom_bar(stat = \"identity\", fill = \"green\") +\n  coord_flip() +\n  labs(title = \"Top 10 Most Common Words in Netflix Titles\",\n       x = \"Words\",\n       y = \"Frequency\")\n\n\n\n\n\n\n\n\nThis data frame and graph show the top 10 most common words used in Netflix titles. I excluded common conjunction words such as “the”, “and”, or “of”. I also used the regular expressions \\d and \\s+ to filter out digits and whitespace. My early prediction of “love”, “life”, “world” all being popular words in titles turned out to be true. However, “death” did not make the top 10."
  },
  {
    "objectID": "MiniProject3.html",
    "href": "MiniProject3.html",
    "title": "Mini Project 3 Final",
    "section": "",
    "text": "Analysis Plan:\nTo test whether home-court advantage has a measurable effect on win percentages, we’ll calculate the observed difference in median win percentage between home and away games across all NCAA teams in our dataset. To determine if this observed difference could happen by chance, we’ll use a permutation test. Specifically, we’ll shuffle the “home” and “away” labels for each team multiple times and calculate the median difference in win percentages after each shuffle. By comparing our observed difference to this distribution of permuted differences, we can assess whether any observed effect is statistically significant.\nThe result will show whether the median win percentage difference between home and away games is greater than what we’d expect by random chance alone. A low p-value would suggest that the difference is meaningful and likely due to a true home-court advantage.\nData Source:\nThe data used in this analysis comes from a Kaggle dataset by Nishaan Amin, titled March Madness Data, which includes statistics on NCAA men’s basketball tournament teams from 2008 to 2024 (excluding 2020, as the tournament was canceled due to COVID-19). Specifically, we used two files: Barttorvik Home.csv and Barttorvik Away.csv, which provide team statistics for home and away games. These files were sourced from Barttorvik’s college basketball data. The full dataset is available on Kaggle: Home file and Away file.\n\nlibrary(tidyverse)\n# Original Data\nhome_data &lt;- read.csv(\"Barttorvik Home.csv\")\naway_data &lt;- read.csv(\"Barttorvik Away.csv\")\n\n\n# Select Relevant Columns and Add Location Column\nhome_data &lt;- home_data |&gt;\n  select(TEAM, WIN.) |&gt;\n  rename(win_pct = WIN.) |&gt;\n  mutate(location = \"home\")\n\naway_data &lt;- away_data |&gt;\n  select(TEAM, WIN.) |&gt;\n  rename(win_pct = WIN.) |&gt;\n  mutate(location = \"away\")\n\n#Group by TEAM and Location, Calculate Median Win Percentage\nhome_median &lt;- home_data |&gt;\n  group_by(TEAM, location) |&gt;\n  summarise(median_win_pct = median(win_pct, na.rm = TRUE))\n\naway_median &lt;- away_data |&gt;\n  group_by(TEAM, location) |&gt;\n  summarise(median_win_pct = median(win_pct, na.rm = TRUE))\n\n# Combine Home and Away Results\ncombined_median_data &lt;- bind_rows(home_median, away_median)\n\n\n#Calculate the Observed Difference in Median Win Percentage\nobserved_diff &lt;- combined_median_data |&gt;\n  group_by(location) |&gt;\n  summarise(median_win_pct = median(median_win_pct, na.rm = TRUE)) |&gt;\n  summarise(diff = diff(median_win_pct)) |&gt;\n  pull(diff)\n\ncat(\"Observed Difference in Median Win Percentage (Home - Away):\", observed_diff, \"\\n\")\n\nObserved Difference in Median Win Percentage (Home - Away): 31.16884 \n\n\n\n# Define Permutation Function (Shuffling location within each team)\ncalculate_permutation &lt;- function(data) {\n  data |&gt;\n    group_by(TEAM) |&gt;\n    mutate(location = sample(location, replace = FALSE)) |&gt;\n    group_by(location) |&gt;\n    summarise(median_win_pct = median(median_win_pct, na.rm = TRUE)) |&gt;\n    summarise(diff = diff(median_win_pct)) |&gt;\n    pull(diff)\n}\n#Permutation Test\nnum_permutations &lt;- 10000\nperm_results &lt;- map_dbl(1:num_permutations, ~ calculate_permutation(combined_median_data))\n\n\np_value &lt;- mean(abs(perm_results) &gt;= abs(observed_diff))\ncat(\"Two-Sided P-value:\", p_value, \"\\n\")\n\nTwo-Sided P-value: 0 \n\n\n\n# Histogram of Permuted Differences with the Observed Difference\nggplot(data.frame(perm_results), aes(x = perm_results)) +\n  geom_histogram(bins = 30, color = \"black\", fill = \"skyblue\") +\n  geom_vline(xintercept = observed_diff, color = \"red\", linetype = \"dashed\", linewidth = 1.2) +\n  labs(\n    title = \"Permutation Test: Distribution of Permuted Differences\",\n    x = \"Difference in Median Win Percentage (Home  - Away)\",\n    y = \"Frequency\"\n  )\n\n\n\n\n\n\n\n\nExplanation of Plot:\nThe histogram shows the distribution of permuted differences in median win percentages between home and away games, simulating what we’d expect to see under the null hypothesis (no real difference between home and away performance). Each bar represents the frequency of permuted differences from our 1,000 random shuffles of the home and away labels. The red dashed line on the far right marks the observed difference in median win percentage, which is approximately 31.17%.\nSummary of Findings:\nThe observed difference in median win percentages between home and away games is 31.17%, indicating that teams tend to perform better at home by a substantial margin. The two-sided p-value of 0 confirms that this observed difference is highly statistically significant. None of the permuted differences reached or exceeded the observed value, strongly suggesting that the observed home-court advantage is not due to random variation.\nThis analysis provides strong evidence that playing at home has a positive and significant impact on NCAA teams’ win percentages. Factors such as crowd support, familiarity with the environment, and the absence of travel fatigue may contribute to this advantage. The findings show that home-court advantage is likely a real and influential factor in game outcomes."
  },
  {
    "objectID": "Project4_SQL.html",
    "href": "Project4_SQL.html",
    "title": "Project 4 - SQL",
    "section": "",
    "text": "Introduction\nThere are two main goals of this project. First, to recreate Figure 1 from Voss (2020) using SQL to query the Wideband Acoustic Immittance (WAI) Database. This requires calculating mean absorbance values for 12 selected studies and producing a correctly labeled plot in R. Second, to explore a study in the database where subjects of varying age categories, were enrolled. For this study, I will analyze frequency vs. mean absorption by age category and create an informative plot to visualize the data.\nThe SQL queries handle data filtering, aggregation, and JOIN operations to generate datasets ready for visualization in R. The analysis aims to demonstrate proficiency in both SQL for data wrangling and R for plotting.\n\n\nSource\nThis project uses data from the Wideband Acoustic Immittance (WAI) Database hosted by Smith College, which provides WAI ear measurements published in peer-reviewed articles. The data was accessed via SQL queries for analysis.\nThe analysis reproduces Figure 1 from the article:\nVoss, S. E., & Allen, J. B. (2020). “Measurement and Analysis of Middle Ear Function in Humans.”\nHearing Research, 384\nAvailable at: https://pmc.ncbi.nlm.nih.gov/articles/PMC7093226/.\n\nEstablish connection to the database and also create variables for needed tables for ease of use.\n\n#Establish Connection\nlibrary(RMariaDB)\nlibrary(tidyverse)\ncon_wai &lt;- dbConnect(\n  MariaDB(), host = \"scidb.smith.edu\",\n  user = \"waiuser\", password = \"smith_waiDB\", \n  dbname = \"wai\"\n)\n\n# Access the relavent tables\nMeasurements &lt;- tbl(con_wai, \"Measurements\")\nPI_Info &lt;- tbl(con_wai, \"PI_Info\")\nSubjects &lt;- tbl(con_wai, \"Subjects\")\n\nA look at all tables in the data set\n\nSHOW TABLES;\n\n\n7 records\n\n\nTables_in_wai\n\n\n\n\nCodebook\n\n\nMeasurements\n\n\nMeasurements_pre2020\n\n\nPI_Info\n\n\nPI_Info_OLD\n\n\nSubjects\n\n\nSubjects_pre2020\n\n\n\n\n\nView the Measurements variables\n\nDESCRIBE Measurements;\n\n\nDisplaying records 1 - 10\n\n\nField\nType\nNull\nKey\nDefault\nExtra\n\n\n\n\nIdentifier\nvarchar(50)\nNO\nPRI\nNA\n\n\n\nSubjectNumber\nint\nNO\nPRI\nNA\n\n\n\nSession\nint\nNO\nPRI\nNA\n\n\n\nEar\nvarchar(50)\nNO\nPRI\n\n\n\n\nInstrument\nvarchar(50)\nNO\nPRI\n\n\n\n\nAge\nfloat\nYES\n\nNA\n\n\n\nAgeCategory\nvarchar(50)\nYES\n\nNA\n\n\n\nEarStatus\nvarchar(50)\nYES\n\nNA\n\n\n\nTPP\nfloat\nYES\n\nNA\n\n\n\nAreaCanal\nfloat\nYES\n\nNA\n\n\n\n\n\n\nView the PI_Info variables\n\nDESCRIBE PI_Info;\n\n\nDisplaying records 1 - 10\n\n\nField\nType\nNull\nKey\nDefault\nExtra\n\n\n\n\nIdentifier\nvarchar(50)\nNO\nPRI\nNA\n\n\n\nYear\nint\nNO\n\nNA\n\n\n\nAuthors\ntext\nNO\n\nNA\n\n\n\nAuthorsShortList\ntext\nNO\n\nNA\n\n\n\nTitle\ntext\nNO\n\nNA\n\n\n\nJournal\ntext\nNO\n\nNA\n\n\n\nURL\ntext\nNO\n\nNA\n\n\n\nAbstract\ntext\nNO\n\nNA\n\n\n\nDataSubmitterName\ntext\nNO\n\nNA\n\n\n\nDataSubmitterEmail\ntext\nNO\n\nNA\n\n\n\n\n\n\nSelecting and viewing relevant columns within a study\n\nSELECT \n  Identifier,\n  Frequency,\n  Absorbance\nFROM Measurements\nWhere Identifier = \"Abur_2014\"\n\nLIMIT 0, 5;\n\n\n5 records\n\n\nIdentifier\nFrequency\nAbsorbance\n\n\n\n\nAbur_2014\n210.938\n0.0333379\n\n\nAbur_2014\n234.375\n0.0315705\n\n\nAbur_2014\n257.812\n0.0405751\n\n\nAbur_2014\n281.250\n0.0438399\n\n\nAbur_2014\n304.688\n0.0486400\n\n\n\n\n\n\nUsed the following queary to create an output variable called “data” to be used as the main data source for our plot in R.\n\n\nSELECT \n  Measurements.Identifier,\n  PI_Info.AuthorsShortList,\n  Measurements.Instrument,\n  Measurements.Frequency,\n  AVG(Measurements.Absorbance) AS MeanAbsorbance,\n  CONCAT(PI_Info.AuthorsShortList, ' et al. N=', \n         COUNT(DISTINCT CONCAT(Measurements.SubjectNumber, Measurements.Ear)), ', ', Measurements.Instrument) AS Legend_Label\nFROM Measurements\nJOIN PI_Info ON Measurements.Identifier = PI_Info.Identifier\nWHERE Measurements.Identifier IN ('Abur_2014', 'Feeney_207', 'Groon_2015', 'Lewis_2015', 'Liu_2008', 'Rosowski_2012', 'Shahnaz_2006', 'Shaver_2013', 'Sun_2016', 'Voss_1994', 'Voss_2010', 'Werner_2010')\nGROUP BY Measurements.Identifier, Measurements.Instrument, PI_Info.AuthorsShortList, Measurements.Frequency;\n\nThe resulting dataset was visualized in R to replicate the mean absorbance plot from Voss (2020):\n\ndata$Frequency &lt;- as.numeric(data$Frequency)\ndata &lt;- data |&gt;\n  filter(Frequency &gt;= 200)\n\nggplot(data, aes(x = Frequency, y = MeanAbsorbance, color = Legend_Label)) +\n  geom_line(size = 0.8) +\n  labs(\n    title = \"Mean absorbance from publication's in WAI database\",\n    x = \"Frequency (Hz)\",\n    y = \"Mean Absorbance\",\n  ) +\n  theme_minimal() +\n  scale_x_continuous(\n    trans = \"log10\",\n    breaks = c(200, 400, 600, 800, 1000, 2000, 4000, 6000, 8000),\n    labels = c(\"200\", \"400\", \"600\", \"800\", \"1000\", \"2000\", \"4000\", \"6000\", \"8000\"),\n    limits = c(200, 8000)\n  ) +\n  scale_y_continuous(\n    limits = c(0, 1),\n    expand = c(0, 0)\n  )\n\n\n\n\n\n\n\n\nDescription:\nThis plot shows mean absorbance values across frequencies for the 12 studies in the WAI database. Each line represents a study, with a legend indicating the authors, the number of unique ears included, and the instrument used.\n\nShows the age categories\n\n\nSELECT DISTINCT AgeCategoryFirstMeasurement\nFROM Subjects;\n\n\n4 records\n\n\nAgeCategoryFirstMeasurement\n\n\n\n\nAdult\n\n\nInfant\n\n\nChild\n\n\nNICU\n\n\n\n\n\n\n\nStudy 1: Abur_2014\nI began by analyzing the Abur_2014 study for differences in absorbance by age category. I used the follwoing queary to create an output variable for this study called “age_data_Abur_2014”\n\nSELECT \n  Subjects.AgeCategoryFirstMeasurement AS AgeCategory, \n  Measurements.Frequency, \n  AVG(Measurements.Absorbance) AS MeanAbsorbance\nFROM Measurements\nJOIN Subjects ON Measurements.SubjectNumber = Subjects.SubjectNumber\nWHERE Measurements.Identifier = 'Abur_2014' -- Adjust based on study of interst\nGROUP BY Subjects.AgeCategoryFirstMeasurement, Measurements.Frequency;\n\nRespective graph for Abur_2014 age data.\n\nggplot(age_data_Abur_2014, aes(x = Frequency, y = MeanAbsorbance, color = AgeCategory)) +\n  geom_line(size = 1) +\n  labs(\n    title = \"Frequency vs. Mean Absorbance by Age Category (Abur_2014)\",\n    x = \"Frequency (Hz)\",\n    y = \"Mean Absorbance\",\n    color = \"Age Category\"\n  ) +\n  theme_minimal() +\n  scale_x_continuous(\n    trans = \"log10\",\n    breaks = c(200, 400, 600, 800, 1000, 2000, 4000, 6000, 8000),\n    labels = c(\"200\", \"400\", \"600\", \"800\", \"1000\", \"2000\", \"4000\", \"6000\", \"8000\"),\n    limits = c(200, 8000)\n  ) +\n  scale_y_continuous(\n    limits = c(0, 1),\n    expand = c(0, 0)\n  ) +\n  theme(\n    legend.position = \"right\",\n    plot.title = element_text(hjust = 0.5)\n  )\n\n\n\n\n\n\n\n\nDescription:\nThis plot illustrates how mean absorbance varies across frequencies for different age categories in the Abur_2014 study. In this study all age groups were found to have nearly identical absorbance across frequencies.\n\nCheck which study included NICU in order to visuilze all age group catagories and view an additional study.\n\nSELECT *\nFROM Subjects\nWHERE AgeCategoryFirstMeasurement = 'NICU'\nLIMIT 100;\n\n\nDisplaying records 1 - 10\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIdentifier\nSubjectNumber\nSessionTotal\nAgeFirstMeasurement\nAgeCategoryFirstMeasurement\nSex\nRace\nEthnicity\nLeftEarStatusFirstMeasurement\nRightEarStatusFirstMeasurement\nSubjectNotes\n\n\n\n\nHunter_2016\n1005\n1\n1.0750000\nNICU\nMale\nCaucasian\nNonHispanic\nNormal\nNormal\nnone\n\n\nHunter_2016\n1026\n5\n0.0000000\nNICU\nFemale\nCaucasian\nNonHispanic\nNormal\nNormal\nnone\n\n\nHunter_2016\n1035\n4\n0.1916670\nNICU\nMale\nCaucasian\nNonHispanic\nNormal\nNormal\nnone\n\n\nHunter_2016\n1046\n2\n0.1500000\nNICU\nFemale\nCaucasian\nNonHispanic\nNormal\nNormal\nnone\n\n\nHunter_2016\n2001\n1\n0.9666670\nNICU\nMale\nCaucasian\nNonHispanic\nNormal\nNormal\nnone\n\n\nHunter_2016\n2006\n2\n0.8083330\nNICU\nMale\nCaucasian\nNonHispanic\nNormal\nNormal\nnone\n\n\nHunter_2016\n2008\n1\n0.8500000\nNICU\nFemale\nCaucasian\nNonHispanic\nNormal\nNormal\nnone\n\n\nHunter_2016\n2014\n2\n0.6500000\nNICU\nFemale\nBlack\nNonHispanic\nNormal\nNormal\nnone\n\n\nHunter_2016\n2030\n2\n0.1666670\nNICU\nFemale\nCaucasian\nNonHispanic\nNormal\nNormal\nnone\n\n\nHunter_2016\n2031\n2\n0.0166667\nNICU\nFemale\nCaucasian\nNonHispanic\nNormal\nNormal\nnone\n\n\n\n\n\n\n\nStudy 2: Hunter_2016 (Including NICU)\nNext, I explored the Hunter_2016 study, which includes NICU participants:\n\nSELECT \n  Subjects.AgeCategoryFirstMeasurement AS AgeCategory, \n  Measurements.Frequency, \n  AVG(Measurements.Absorbance) AS MeanAbsorbance\nFROM Measurements\nJOIN Subjects ON Measurements.SubjectNumber = Subjects.SubjectNumber\nWHERE Measurements.Identifier = 'Hunter_2016' -- Adjust to your study of interest\nGROUP BY Subjects.AgeCategoryFirstMeasurement, Measurements.Frequency;\n\nRespective graph for Hunter_2016 age data.\n\nggplot(age_data_Hunter_2016, aes(x = Frequency, y = MeanAbsorbance, color = AgeCategory)) +\n  geom_line(size = 1) +\n  labs(\n    title = \"Frequency vs. Mean Absorbance by Age Category (Hunter_2016)\",\n    x = \"Frequency (Hz)\",\n    y = \"Mean Absorbance\",\n    color = \"Age Category\"\n  ) +\n  theme_minimal() +\n  scale_x_continuous(\n    trans = \"log10\",\n    breaks = c(200, 400, 600, 800, 1000, 2000, 4000, 6000, 8000),\n    labels = c(\"200\", \"400\", \"600\", \"800\", \"1000\", \"2000\", \"4000\", \"6000\", \"8000\"),\n    limits = c(200, 8000)\n  ) +\n  scale_y_continuous(\n    limits = c(0, 1),\n    expand = c(0, 0)\n  ) +\n  theme(\n    legend.position = \"right\",\n    plot.title = element_text(hjust = 0.5)\n  )\n\n\n\n\n\n\n\n\nDescription:\nThis plot highlights the variation in absorbance across frequencies for the Hunter_2016 study. The NICU age group is included, and their absorbance patterns are compared with other age categories. In this study it was found that adults had significantly less absorbance im comparison to younger age groups.\n\n\n\nConclusion\nThis analysis successfully replicated Figure 1 from Voss (2020) by querying the WAI database and visualizing mean absorbance trends across studies. It also explored absorbance differences by age category for two studies, providing insight into how absorbance varies for different populations, including NICU. The project demonstrates the power of SQL for efficient data wrangling and R for effective visualization."
  },
  {
    "objectID": "Project4_SQL.html#introduction",
    "href": "Project4_SQL.html#introduction",
    "title": "Project 4 - SQL",
    "section": "",
    "text": "There are two main goals of this project. First, to recreate Figure 1 from Voss (2020) using SQL to query the Wideband Acoustic Immittance (WAI) Database. This requires calculating mean absorbance values for 12 selected studies and producing a correctly labeled plot in R. Second, to explore a study in the database where subjects of varying age categories, were enrolled. For this study, I will analyze frequency vs. mean absorption by age category and create an informative plot to visualize the data.\nThe SQL queries handle data filtering, aggregation, and JOIN operations to generate datasets ready for visualization in R. The analysis aims to demonstrate proficiency in both SQL for data wrangling and R for plotting.\nEstablish connection to the database and also create variables for needed tables for ease of use.\n\n#Establish Connection\nlibrary(RMariaDB)\nlibrary(tidyverse)\ncon_wai &lt;- dbConnect(\n  MariaDB(), host = \"scidb.smith.edu\",\n  user = \"waiuser\", password = \"smith_waiDB\", \n  dbname = \"wai\"\n)\n\n# Access the relavent tables\nMeasurements &lt;- tbl(con_wai, \"Measurements\")\nPI_Info &lt;- tbl(con_wai, \"PI_Info\")\nSubjects &lt;- tbl(con_wai, \"Subjects\")\n\nA look at all tables in the data set\n\nSHOW TABLES;\n\n\n7 records\n\n\nTables_in_wai\n\n\n\n\nCodebook\n\n\nMeasurements\n\n\nMeasurements_pre2020\n\n\nPI_Info\n\n\nPI_Info_OLD\n\n\nSubjects\n\n\nSubjects_pre2020\n\n\n\n\n\nView the Measurements variables\n\nDESCRIBE Measurements;\n\n\nDisplaying records 1 - 10\n\n\nField\nType\nNull\nKey\nDefault\nExtra\n\n\n\n\nIdentifier\nvarchar(50)\nNO\nPRI\nNA\n\n\n\nSubjectNumber\nint\nNO\nPRI\nNA\n\n\n\nSession\nint\nNO\nPRI\nNA\n\n\n\nEar\nvarchar(50)\nNO\nPRI\n\n\n\n\nInstrument\nvarchar(50)\nNO\nPRI\n\n\n\n\nAge\nfloat\nYES\n\nNA\n\n\n\nAgeCategory\nvarchar(50)\nYES\n\nNA\n\n\n\nEarStatus\nvarchar(50)\nYES\n\nNA\n\n\n\nTPP\nfloat\nYES\n\nNA\n\n\n\nAreaCanal\nfloat\nYES\n\nNA\n\n\n\n\n\n\nView the PI_Info variables\n\nDESCRIBE PI_Info;\n\n\nDisplaying records 1 - 10\n\n\nField\nType\nNull\nKey\nDefault\nExtra\n\n\n\n\nIdentifier\nvarchar(50)\nNO\nPRI\nNA\n\n\n\nYear\nint\nNO\n\nNA\n\n\n\nAuthors\ntext\nNO\n\nNA\n\n\n\nAuthorsShortList\ntext\nNO\n\nNA\n\n\n\nTitle\ntext\nNO\n\nNA\n\n\n\nJournal\ntext\nNO\n\nNA\n\n\n\nURL\ntext\nNO\n\nNA\n\n\n\nAbstract\ntext\nNO\n\nNA\n\n\n\nDataSubmitterName\ntext\nNO\n\nNA\n\n\n\nDataSubmitterEmail\ntext\nNO\n\nNA\n\n\n\n\n\n\nSelecting and viewing relevant columns within a study\n\nSELECT \n  Identifier,\n  Frequency,\n  Absorbance\nFROM Measurements\nWhere Identifier = \"Abur_2014\"\n\nLIMIT 0, 5;\n\n\n5 records\n\n\nIdentifier\nFrequency\nAbsorbance\n\n\n\n\nAbur_2014\n210.938\n0.0333379\n\n\nAbur_2014\n234.375\n0.0315705\n\n\nAbur_2014\n257.812\n0.0405751\n\n\nAbur_2014\n281.250\n0.0438399\n\n\nAbur_2014\n304.688\n0.0486400\n\n\n\n\n\nThe following query calculates mean absorbance values for the 12 studies in Voss (2020), along with data for creating a descriptive plot legend:\n\n\nSELECT \n  Measurements.Identifier,\n  PI_Info.AuthorsShortList,\n  Measurements.Instrument,\n  Measurements.Frequency,\n  AVG(Measurements.Absorbance) AS MeanAbsorbance,\n  CONCAT(PI_Info.AuthorsShortList, ' et al. N=', \n         COUNT(DISTINCT CONCAT(Measurements.SubjectNumber, Measurements.Ear)), ', ', Measurements.Instrument) AS Legend_Label\nFROM Measurements\nJOIN PI_Info ON Measurements.Identifier = PI_Info.Identifier\nWHERE Measurements.Identifier IN ('Abur_2014', 'Feeney_207', 'Groon_2015', 'Lewis_2015', 'Liu_2008', 'Rosowski_2012', 'Shahnaz_2006', 'Shaver_2013', 'Sun_2016', 'Voss_1994', 'Voss_2010', 'Werner_2010')\nGROUP BY Measurements.Identifier, Measurements.Instrument, PI_Info.AuthorsShortList, Measurements.Frequency;\n\n\nDisplaying records 1 - 10\n\n\n\n\n\n\n\n\n\n\nIdentifier\nAuthorsShortList\nInstrument\nFrequency\nMeanAbsorbance\nLegend_Label\n\n\n\n\nAbur_2014\nAbur et al.\nHearID\n210.938\n0.0784746\nAbur et al. et al. N=14, HearID\n\n\nAbur_2014\nAbur et al.\nHearID\n234.375\n0.0826420\nAbur et al. et al. N=14, HearID\n\n\nAbur_2014\nAbur et al.\nHearID\n257.812\n0.0948482\nAbur et al. et al. N=14, HearID\n\n\nAbur_2014\nAbur et al.\nHearID\n281.250\n0.1031472\nAbur et al. et al. N=14, HearID\n\n\nAbur_2014\nAbur et al.\nHearID\n304.688\n0.1137576\nAbur et al. et al. N=14, HearID\n\n\nAbur_2014\nAbur et al.\nHearID\n328.125\n0.1221205\nAbur et al. et al. N=14, HearID\n\n\nAbur_2014\nAbur et al.\nHearID\n351.562\n0.1334329\nAbur et al. et al. N=14, HearID\n\n\nAbur_2014\nAbur et al.\nHearID\n375.000\n0.1447725\nAbur et al. et al. N=14, HearID\n\n\nAbur_2014\nAbur et al.\nHearID\n398.438\n0.1563874\nAbur et al. et al. N=14, HearID\n\n\nAbur_2014\nAbur et al.\nHearID\n421.875\n0.1806973\nAbur et al. et al. N=14, HearID\n\n\n\n\n\nTurns the previous query into an output variable called “data” to be used as the main data source for our plot in R.\n\n\nSELECT \n  Measurements.Identifier,\n  PI_Info.AuthorsShortList,\n  Measurements.Instrument,\n  Measurements.Frequency,\n  AVG(Measurements.Absorbance) AS MeanAbsorbance,\n  CONCAT(PI_Info.AuthorsShortList, ' et al. N=', \n         COUNT(DISTINCT CONCAT(Measurements.SubjectNumber, Measurements.Ear)), ', ', Measurements.Instrument) AS Legend_Label\nFROM Measurements\nJOIN PI_Info ON Measurements.Identifier = PI_Info.Identifier\nWHERE Measurements.Identifier IN ('Abur_2014', 'Feeney_207', 'Groon_2015', 'Lewis_2015', 'Liu_2008', 'Rosowski_2012', 'Shahnaz_2006', 'Shaver_2013', 'Sun_2016', 'Voss_1994', 'Voss_2010', 'Werner_2010')\nGROUP BY Measurements.Identifier, Measurements.Instrument, PI_Info.AuthorsShortList, Measurements.Frequency;\n\nThe resulting dataset was visualized in R to replicate the mean absorbance plot from Voss (2020):\n\ndata$Frequency &lt;- as.numeric(data$Frequency)\ndata &lt;- data |&gt;\n  filter(Frequency &gt;= 200)\n\nggplot(data, aes(x = Frequency, y = MeanAbsorbance, color = Legend_Label)) +\n  geom_line(size = 0.8) +\n  labs(\n    title = \"Mean absorbance from publication's in WAI database\",\n    x = \"Frequency (Hz)\",\n    y = \"Mean Absorbance\",\n  ) +\n  theme_minimal() +\n  scale_x_continuous(\n    trans = \"log10\",\n    breaks = c(200, 400, 600, 800, 1000, 2000, 4000, 6000, 8000),\n    labels = c(\"200\", \"400\", \"600\", \"800\", \"1000\", \"2000\", \"4000\", \"6000\", \"8000\"),\n    limits = c(200, 8000)\n  ) +\n  scale_y_continuous(\n    limits = c(0, 1),\n    expand = c(0, 0)\n  )"
  }
]